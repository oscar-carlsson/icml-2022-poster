
@article{aronsson2021,
    title = {Homogeneous vector bundles and {{$G$-equivariant Convolutional Neural Networks}}},
    author = {Aronsson, Jimmy},
    year = {2021},
    month = may,
    archiveprefix = {arXiv},
    eprint = {arXiv:2105.05400},
    journal = {arXiv:2105.05400, to appear in Sampling Theory, Signal Processing, and Data Analysis},
    eprinttype = {arxiv},
    keywords = {}
}

@article{bogatskiy2020,
    title = {Lorentz {{ Equivariant Neural Network}} for {{Particle Physics}}},
    author = {Bogatskiy, Alexander and Anderson, Brandon and Offermann, Jan T. and Roussi, Marwah and Miller, David W. and Kondor, Risi},
    archiveprefix = {arXiv},
    eprint = {2006.04780},
    eprinttype = {arxiv},
    year = {2020},
    keywords = {Computer Science - Machine Learning,High Energy Physics - Experiment,High Energy Physics - Phenomenology,Physics - Computational Physics,Statistics - Machine Learning},
    primaryclass = {hep-ex, physics:hep-ph, physics:physics, stat}
}

@inproceedings{boomsma2017,
    title = {Spherical Convolutions and Their Application in Molecular Modelling},
    booktitle = {Advances in Neural Information Processing Systems},
    author = {Boomsma, Wouter and Frellsen, Jes},
    editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
    year = {2017},
    volume = {30},
    pages = {3433--3443},
    publisher = {{Curran Associates, Inc.}},
    url = {https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf}
}

@inproceedings{Cohen2016,
    author = {{Cohen}, Taco S. and {Welling}, Max},
    title = "{Group Equivariant Convolutional Networks}",
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
    pages = 	 {2990--2999},
    year = 	 {2016},
    editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
    volume = 	 {48},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {New York, New York, USA},
    month = 	 {20--22 Jun},
    publisher =    {PMLR},
    archivePrefix = {arXiv},
    eprint = {1602.07576},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207576C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{cohen2018,
    ids = {cohen2020},
    title = {A {{General Theory}} of {{Equivariant CNNs}} on {{Homogeneous Spaces}}},
    author = {Cohen, Taco and Geiger, Mario and Weiler, Maurice},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    volume = {32},
    year = {2019},
    month = dec,
    archiveprefix = {arXiv},
    eprint = {1811.02017},
    eprinttype = {arxiv},
}

@inproceedings{cohen2018b,
    author    = {Taco S. Cohen and
        Mario Geiger and
        Jonas K{\"{o}}hler and
        Max Welling},
    title     = {{S}pherical {CNNs}},
    booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
        Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
    publisher = {OpenReview.net},
    year      = {2018},
    timestamp = {Thu, 21 Jan 2021 17:36:45 +0100},
    biburl    = {https://dblp.org/rec/conf/iclr/CohenGKW18.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cohen2018b_old,
    title = {Spherical {{CNNs}}},
    author = {Cohen, Taco S. and Geiger, Mario and K\"ohler, Jonas and Welling, Max},
    booktitle={International Conference on Learning Representations},
    year = {2018},
    eprint = {1801.10130},
    eprinttype = {arxiv},
    archiveprefix = {arXiv},
    eventtitle = {International {{Conference}} on {{Learning Representations}}},
    keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
    langid = {english}
}

@inproceedings{CohenGauge2019,
    author = {{Cohen}, Taco S. and {Weiler}, Maurice and {Kicanaoglu}, Berkay and {Welling}, Max},
    title = "{Gauge Equivariant Convolutional Networks and the Icosahedral CNN}",
    booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
    pages = 	 {1321--1330},
    year = 	 {2019},
    editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
    volume = 	 {97},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {09--15 Jun},
    publisher =    {PMLR},
    eprint = {1902.04615},
    eprinttype = {arxiv},
    archiveprefix = {arXiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190204615C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{CohenChengGauge2019,
    author = {{Cheng}, Miranda C.~N. and {Anagiannis}, Vassilis and {Weiler}, Maurice and {de Haan}, Pim and {Cohen}, Taco S. and {Welling}, Max},
    title = "{Covariance in Physics and Convolutional Neural Networks}",
    keywords = {Computer Science - Machine Learning, High Energy Physics - Theory, Statistics - Machine Learning},
    year = 2019,
    month = jun,
    archiveprefix = {arXiv},
    eprinttype = {arXiv},
    eprint = {1906.02481},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602481C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{CohenMesh,
    author = {{de Haan}, Pim and {Weiler}, Maurice and {Cohen}, Taco and {Welling}, Max},
    title = "{Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs}",
    keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
    year = 2020,
    month = mar,
    eprinttype = {arXiv},
    archiveprefix = {arXiv},
    eprint = {2003.05425},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200305425D},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{coors2018,
    title = {{{SphereNet}}: {{Learning}} Spherical Representations for Detection and Classification in Omnidirectional Images},
    shorttitle = {{{SphereNet}}},
    booktitle = {Computer Vision \textendash{} {{ECCV}} 2018},
    author = {Coors, Benjamin and Condurache, Alexandru Paul and Geiger, Andreas},
    editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
    year = {2018},
    pages = {525--541},
    publisher = {{Springer International Publishing}},
    address = {{Cham}},
    doi = {10.1007/978-3-030-01240-3_32},
    isbn = {978-3-030-01240-3}
}

@inproceedings{defferrard2020deepsphere,
    title = {{{DeepSphere}}: A Graph-Based Spherical {{CNN}}},
    booktitle = {International Conference on Learning Representations},
    author = {Defferrard, Micha\"el and Milani, Martino and Gusset, Fr\'ed\'erick and Perraudin, Nathana\"el},
    year = {2020},
    archiveprefix = {arXiv},
    eprint = {2012.15000},
    eprinttype = {arxiv}
}

@inproceedings{esteves2018,
    title = {Learning {{SO}}(3) {{Equivariant Representations}} with {{Spherical CNNs}}},
    booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
    author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
    year = {2018},
    pages = {52--68},
    archiveprefix = {arXiv},
    eprint = {1711.06721},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{esteves2020,
    title = {Theoretical {{Aspects}} of {{Group Equivariant Neural Networks}}},
    author = {Esteves, Carlos},
    archiveprefix = {arXiv},
    eprint = {2004.05154},
    journal = {arXiv:2004.05154 [cs, stat]},
    year = {2020},
    month = apr,
    eprinttype = {arxiv}
}

@inproceedings{Kondor2018,
    author = {{Kondor}, Risi and {Trivedi}, Shubhendu},
    title = "{On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups}",
    booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
    pages = 	 {2747--2755},
    year = 	 {2018},
    editor = 	 {Dy, Jennifer and Krause, Andreas},
    volume = 	 {80},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {10--15 Jul},
    publisher =    {PMLR},
    archivePrefix = {arXiv},
    eprint = {1802.03690},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180203690K},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{lang2020,
    title = {A {{Wigner}}-{{Eckart Theorem}} for {{Group Equivariant Convolution Kernels}}},
    author = {Lang, Leon and Weiler, Maurice},
    archiveprefix = {arXiv},
    eprint = {2010.10952},
    year = {2020},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{linmans2018,
    title = {{S}ample {E}fficient {S}emantic {S}egmentation using {R}otation {E}quivariant {C}onvolutional {N}etworks},
    author = {Linmans, Jasper and Winkens, Jim and Veeling, Bastian S. and Cohen, Taco S. and Welling, Max},
    booktitle={International Conference on Machine Learning},
    year = {2018},
    month = jul,
    archiveprefix = {arXiv},
    eprint = {arXiv:1807.00583},
    journal = {International Conference on Machine Learning Workshop “Towards learning with limited labels: Equivariance, Invariance, and Beyond”},
    eprinttype = {arxiv},
    keywords = {},
}

@inproceedings{2017arXiv170603762V,
    author = {{Vaswani}, Ashish and {Shazeer}, Noam and {Parmar}, Niki and {Uszkoreit}, Jakob and {Jones}, Llion and {Gomez}, Aidan N. and {Kaiser}, Lukasz and {Polosukhin}, Illia},
    title = "{Attention Is All You Need}",
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
    archivePrefix = {arXiv},
    eprint = {1706.03762},
    eprinttype = {arxiv},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603762V},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{2020arXiv200203830R,
    author = {{Romero}, David W. and {Bekkers}, Erik J. and {Tomczak}, Jakub M. and {Hoogendoorn}, Mark},
    title = "{Attentive Group Equivariant Convolutional Networks}",
    journal = {arXiv e-prints},
    booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
    pages = 	 {8188--8199},
    editor = 	 {Hal Daumé III and Aarti Singh},
    volume = 	 {119},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {13--18 Jul},
    publisher =    {PMLR},
    year = 2020,
    archivePrefix = {arXiv},
    eprint = {2002.03830},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200203830R},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{2020arXiv200501683D,
    author = {{Dey}, Neel and {Chen}, Antong and {Ghafurian}, Soheil},
    title = "{Group Equivariant Generative Adversarial Networks}",
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
    year = 2021,
    month = may,
    booktitle={International Conference on Learning Representations},
    archivePrefix = {arXiv},
    eprint = {2005.01683},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200501683D},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{LeCunGeometric,
    author = {{Bronstein}, Michael M. and {Bruna}, Joan and {LeCun}, Yann and {Szlam}, Arthur and {Vandergheynst}, Pierre},
    title = "{Geometric Deep Learning: Going beyond Euclidean data}",
    journal = {IEEE Signal Processing Magazine},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    year = 2017,
    month = jul,
    volume = {34},
    number = {4},
    pages = {18-42},
    doi = {10.1109/MSP.2017.2693418},
    archivePrefix = {arXiv},
    eprint = {1611.08097},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2017ISPM...34...18B},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{makadia2007,
    title = {Correspondence-Free {{Structure}} from {{Motion}}},
    author = {Makadia, Ameesh and Geyer, Christopher and Daniilidis, Kostas},
    date = {2007-12-01},
    year = {2007},
    journal = {International Journal of Computer Vision},
    shortjournal = {Int J Comput Vis},
    volume = {75},
    pages = {311--327},
    issn = {1573-1405},
    doi = {10.1007/s11263-007-0035-2},
    langid = {english},
    number = {3}
}

@article{perraudin2019,
    title = {{{DeepSphere}}: {{Efficient}} Spherical {{Convolutional Neural Network}} with {{HEALPix}} Sampling for Cosmological Applications},
    shorttitle = {{{DeepSphere}}},
    author = {Perraudin, Nathana\"el and Defferrard, Micha\"el and Kacprzak, Tomasz and Sgier, Raphael},
    date = {2019-04-01},
    year = {2019},
    journal = {Astronomy and Computing},
    volume = {27},
    pages = {130--146},
    issn = {2213-1337},
    doi = {10.1016/j.ascom.2019.03.004},
    archiveprefix = {arXiv},
    eprint = {1810.12186},
    eprinttype = {arxiv},
    langid = {english}
}

@book{varshalovich1988,
    title = {Quantum {{Theory}} of {{Angular Momentum}}},
    author = {Varshalovich, D A and Moskalev, A N and Khersonskii, V K},
    year = {1988},
    month = {oct},
    publisher = {{WORLD SCIENTIFIC}},
    doi = {10.1142/0270},
    isbn = {978-9971-5-0107-5 978-981-4415-49-1},
    langid = {english}
}

@inproceedings{weiler2018,
    title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
    shorttitle = {{{3D Steerable CNNs}}},
    author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
    shortjournal = {ArXiv180702547 Cs Stat},
    archiveprefix = {arXiv},
    eprint = {1807.02547},
    eprinttype = {arxiv},
    year = {2018},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    pages = {10402–10413},
    numpages = {12},
    location = {Montr\'{e}al, Canada},
    series = {NIPS'18}
}

@inproceedings{weiler2019,
    title = {General {{$E(2)$}}-{{Equivariant Steerable CNNs}}},
    author = {Weiler, Maurice and Cesa, Gabriele},
    archiveprefix = {arXiv},
    eprint = {1911.08251},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    volume = {32},
    year = {2019}
}

@book{deitmar2014principles,
    title={Principles of harmonic analysis},
    author={Deitmar, Anton and Echterhoff, Siegfried},
    year={2014},
    publisher={Springer}
}

@book{lee2013smooth,
    title={Introduction to Smooth Manifolds},
    author={Lee, John M},
    year={2013},
    publisher={Springer}
}

@book{steenrod1999topology,
    title={The topology of fibre bundles},
    author={Steenrod, Norman},
    year={1999},
    publisher={Princeton university press}
}

@book{nakaharaGeometryTopologyPhysics2007,
    title = {Geometry, Topology and Physics},
    author = {Nakahara, M.},
    year = {2003},
    month = jun,
    edition = {2 edition},
    publisher = {{CRC Press}},
    address = {{Bristol ; Philadelphia}},
    isbn = {978-0-7503-0606-5},
    language = {en}
}

@book{wallach2018harmonic,
    title={Harmonic analysis on homogeneous spaces},
    author={Wallach, Nolan R},
    year={2018},
    publisher={Courier Dover Publications}
}


@article{cohenIntertwinersInducedRepresentations2018a,
    title = {Intertwiners between {{Induced Representations}} (with {{Applications}} to the {{Theory}} of {{Equivariant Neural Networks}})},
    author = {Cohen, Taco S. and Geiger, Mario and Weiler, Maurice},
    year = {2018},
    month = mar,
    abstract = {Group equivariant and steerable convolutional neural networks (regular and steerable G-CNNs) have recently emerged as a very effective model class for learning from signal data such as 2D and 3D images, video, and other data where symmetries are present. In geometrical terms, regular G-CNNs represent data in terms of scalar fields ("feature channels"), whereas the steerable G-CNN can also use vector or tensor fields ("capsules") to represent data. In algebraic terms, the feature spaces in regular G-CNNs transform according to a regular representation of the group G, whereas the feature spaces in Steerable G-CNNs transform according to the more general induced representations of G. In order to make the network equivariant, each layer in a G-CNN is required to intertwine between the induced representations associated with its input and output space. In this paper we present a general mathematical framework for G-CNNs on homogeneous spaces like Euclidean space or the sphere. We show, using elementary methods, that the layers of an equivariant network are convolutional if and only if the input and output feature spaces transform according to an induced representation. This result, which follows from G.W. Mackey's abstract theory on induced representations, establishes G-CNNs as a universal class of equivariant network architectures, and generalizes the important recent work of Kondor \& Trivedi on the intertwiners between regular representations.},
    archiveprefix = {arXiv},
    eprint = {1803.10743},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}



@misc{schullerMathsPhysicsFrederic2016,
    title = {Maths with {{Physics}}: {{Frederic Schuller}}'s {{Lectures}} on the {{Geometric Anatomy}} of {{Theoretical Physics}}},
    shorttitle = {Maths with {{Physics}}},
    author = {Schuller, Frederic P.},
    year = {2016},
    month = jul,
    journal = {Maths with Physics}
}

@inproceedings{kondor2018a,
    title = {Clebsch-{{Gordan Nets}}: A {{Fully Fourier Space Spherical Convolutional Neural Network}}},
    shorttitle = {Clebsch-{{Gordan Nets}}},
    author = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    volume = {31},
    year = {2018},
    month = nov,
    archiveprefix = {arXiv},
    eprint = {1806.09231},
    eprinttype = {arxiv},
    keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{cobb2020,
    author = {O.~J.~Cobb and C.~G.~R.~Wallis and A.~N.~Mavor-Parker and A.~Marignier and M.A.~Price and M.~d’Avezac and J.~D.~McEwen},
    booktitle = {International Conference on Learning Representations},
    eprint = {arXiv:2010.11661},
    featured = {true},
    month = {Feb},
    pdf = {papers/efficient_generalized_s2cnn.pdf},
    projects = {fourpiai},
    pubtype = {refereed},
    title = {Efficient generalized spherical CNNs},
    year = {2021}
}

@inproceedings{cobb2020_old,
    title = {Efficient {{Generalized Spherical CNNs}}},
    author = {Cobb, Oliver and Wallis, Christopher G. R. and Mavor-Parker, Augustine N. and Marignier, Augustin and Price, Matthew A. and d' Avezac, Mayeul and McEwen, Jason},
    options = {useprefix=true},
    date = {2020-09-28},
    year = {2020},
    url = {https://openreview.net/forum?id=rWZz3sJfCkm},
    urldate = {2022-01-26},
    abstract = {Many problems across computer vision and the natural sciences require the analysis of spherical data, for which representations may be learned efficiently by encoding equivariance to rotational...},
    eventtitle = {International {{Conference}} on {{Learning Representations}}},
    langid = {english},
    file = {/home/oscar/Zotero/storage/249KDSMH/Cobb et al. - 2020 - Efficient Generalized Spherical CNNs.pdf;/home/oscar/Zotero/storage/TBSSXK6L/forum.html}
}

@article{cobb2020_old_old,
    title = {Efficient {{Generalized Spherical CNNs}}},
    author = {Cobb, Oliver J. and Wallis, Christopher G. R. and {Mavor-Parker}, Augustine N. and Marignier, Augustin and Price, Matthew A. and {d'Avezac}, Mayeul and McEwen, Jason D.},
    year = {2020},
    month = oct,
    archiveprefix = {arXiv},
    eprint = {2010.11661},
    journal = {arXiv:2010.11661 [cs, astro-ph]},
    eprinttype = {arxiv},
    keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{elaldi2021,
    title = {Equivariant {{Spherical Deconvolution}}: {{Learning Sparse Orientation Distribution Functions}} from {{Spherical Data}}},
    shorttitle = {Equivariant {{Spherical Deconvolution}}},
    author = {Elaldi, Axel and Dey, Neel and Kim, Heejong and Gerig, Guido},
    year = {2021},
    month = feb,
    archiveprefix = {arXiv},
    eprint = {2102.09462},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@inproceedings{esteves2020b,
    title = {Spin-{{Weighted Spherical CNNs}}},
    author = {Esteves, Carlos and Makadia, Ameesh and Daniilidis, Kostas},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
    pages = {8614--8625},
    publisher = {Curran Associates, Inc.},
    volume = {33},
    year = {2020},
    month = oct,
    archiveprefix = {arXiv},
    eprint = {2006.10731},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{fox2021,
    title = {Concentric {{Spherical GNN}} for {{3D Representation Learning}}},
    author = {Fox, James and Zhao, Bo and Rajamanickam, Sivasankaran and Ramprasad, Rampi and Song, Le},
    year = {2021},
    month = mar,
    archiveprefix = {arXiv},
    eprint = {2103.10484},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{jiang2018,
    title = {Spherical {{CNNs}} on {{Unstructured Grids}}},
    booktitle = {International {{Conference}} on {{Learning Representations}}},
    author = {Jiang, Chiyu Max and Huang, Jingwei and Kashinath, Karthik and Prabhat and Marcus, Philip and Niessner, Matthias},
    year = {2018},
    month = sep,
    language = {en}
}

@article{mcewen2021,
    title = {Scattering {{Networks}} on the {{Sphere}} for {{Scalable}} and {{Rotationally Equivariant Spherical CNNs}}},
    author = {McEwen, Jason D. and Wallis, Christopher G. R. and {Mavor-Parker}, Augustine N.},
    year = {2021},
    month = feb,
    archiveprefix = {arXiv},
    eprint = {2102.02828},
    eprinttype = {arxiv},
    keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@article{shen2021,
    title = {{{PDO}}-E$\mathrm{S}^2${{CNNs}}: {{Partial Differential Operator Based Equivariant Spherical CNNs}}},
    shorttitle = {{{PDO}}-E$\mathrm{S}^2${{CNNs}}},
    author = {Shen, Zhengyang and Shen, Tiancheng and Lin, Zhouchen and Ma, Jinwen},
    year = {2021},
    month = apr,
    archiveprefix = {arXiv},
    eprint = {2104.03584},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{su2017,
    title = {Learning Spherical Convolution for Fast Features from 360\textdegree\ {{Imagery}}},
    shorttitle = {{{Flat2Sphere}}},
    booktitle = {Advances in Neural Information Processing Systems},
    author = {Su, Yu-Chuan and Grauman, Kristen},
    editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
    year = {2017},
    volume = {30},
    publisher = {{Curran Associates, Inc.}},
    archiveprefix = {arXiv},
    eprint = {1708.00919},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{monroy2018,
    title = {{{SalNet360}}: {{Saliency}} Maps for Omni-Directional Images with {{CNN}}},
    shorttitle = {{{SalNet360}}},
    author = {Monroy, Rafael and Lutz, Sebastian and Chalasani, Tejo and Smolic, Aljosa},
    year = {2018},
    month = nov,
    volume = {69},
    pages = {26--34},
    issn = {0923-5965},
    doi = {10.1016/j.image.2018.05.005},
    journal = {Signal Processing: Image Communication},
    keywords = {Convolutional Neural Network (CNN),Omnidirectional Image (ODI),Saliency,Virtual Reality (VR)},
    language = {en},
    series = {Salient360: {{Visual}} Attention Modeling for 360\textdegree{} {{Images}}}
}

@inproceedings{winkels2018,
    title = {{{3D G}}-{{CNNs}} for {{Pulmonary Nodule Detection}}},
    author = {Winkels, Marysia and Cohen, Taco S.},
    booktitle={International conference on Medical Imaging with Deep Learning},
    year = {2018},
    month = apr,
    archiveprefix = {arXiv},
    eprint = {1804.04656},
    eprinttype = {arxiv},
    keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{thomas2018,
    title = {Tensor Field Networks: {{Rotation}}- and Translation-Equivariant Neural Networks for {{3D}} Point Clouds},
    shorttitle = {Tensor Field Networks},
    author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
    year = {2018},
    month = may,
    archiveprefix = {arXiv},
    eprint = {1802.08219},
    eprinttype = {arxiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{worrall2018,
    title = {{{CubeNet}}: {{Equivariance}} to {{3D Rotation}} and {{Translation}}},
    shorttitle = {{{CubeNet}}},
    booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
    author = {Worrall, Daniel and Brostow, Gabriel},
    year = {2018},
    pages = {567--584},
    archiveprefix = {arXiv},
    eprint = {1804.04458},
    eprinttype = {arxiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}
@article{2020arXiv200601570W,
    author = {{Wiersma}, Ruben and {Eisemann}, Elmar and {Hildebrandt}, Klaus},
    title = "{CNNs on Surfaces using Rotation-Equivariant Features}",
    year = {2020},
    issue_date = {July 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {39},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/3386569.3392437},
    doi = {10.1145/3386569.3392437},
    journal = {ACM Trans. Graph.},
    month = jul,
    articleno = {92},
    numpages = {12},
    archivePrefix = {arXiv},
    eprint = {2006.01570},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200601570W},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{kondor2018b,
    title = {N-Body {{Networks}}: A {{Covariant Hierarchical Neural Network Architecture}} for {{Learning Atomic Potentials}}},
    shorttitle = {N-Body {{Networks}}},
    author = {Kondor, Risi},
    year = {2018},
    month = mar,
    archiveprefix = {arXiv},
    eprint = {1803.01588},
    eprinttype = {arxiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{worrall2017,
    title = {Harmonic {{Networks}}: {{Deep Translation}} and {{Rotation Equivariance}}},
    shorttitle = {Harmonic {{Networks}}},
    booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    author = {Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},
    year = {2017},
    month = jul,
    pages = {7168--7177},
    doi = {10.1109/CVPR.2017.758},
    archiveprefix = {arXiv},
    eprint = {1612.04642},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer vision,Detectors,Filtering theory,Harmonic analysis,Maximum likelihood detection,Nonlinear filters,Power harmonic filters,Statistics - Machine Learning}
}

@article{muller2021,
    title = {Rotation-{{Equivariant Deep Learning}} for {{Diffusion MRI}}},
    author = {M{\"u}ller, Philip and Golkov, Vladimir and Tomassini, Valentina and Cremers, Daniel},
    date = {2021-02-13},
    year = {2021},
    month = feb,
    archiveprefix = {arXiv},
    eprint = {2102.06942},
    journal = {arXiv:2102.06942 [cs]},
    eprinttype = {arxiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{bronstein2021,
    title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
    shorttitle = {Geometric {{Deep Learning}}},
    author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
    year = {2021},
    month = apr,
    archiveprefix = {arXiv},
    eprint = {2104.13478},
    journal = {arXiv:2104.13478 [cs, stat]},
    eprinttype = {arxiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{Favoni:2020reg,
    author = {Favoni, Matteo and Ipp, Andreas and M\"uller, David I. and Schuh, Daniel},
    title = "{Lattice gauge equivariant convolutional neural networks}",
    eprint = "2012.12901",
    archivePrefix = "arXiv",
    eprinttype = {arxiv},
    month = "12",
    year = "2020"
}

@article{Luo:2020stn,
    author = "Luo, Di and Carleo, Giuseppe and Clark, Bryan K. and Stokes, James",
    title = "{Gauge equivariant neural networks for quantum lattice gauge theories}",
    eprint = "2012.05232",
    eprinttype = {arxiv},
    archivePrefix = "arXiv",
    month = "12",
    year = "2020"
}

@book{kolarNaturalOperationsDifferential1993,
    title = {Natural {{Operations}} in {{Differential Geometry}}},
    author = {Kolar, Ivan and Michor, Peter W. and Slovak, Jan},
    year = {1993},
    edition = {1993rd edition},
    publisher = {{Springer}},
    location = {{Berlin ; New York}},
    abstract = {The aim of this work is threefold: First it should be a monographical work on natural bundles and natural operators in differential geometry. This is a field which every differential geometer has met several times, but which is not treated in detail in one place. Let us explain a little, what we mean by naturality. Exterior derivative commutes with the pullback of differential forms. In the background of this statement are the following general concepts. The vector bundle A kT* M is in fact the value of a functor, which associates a bundle over M to each manifold M and a vector bundle homomorphism over f to each local diffeomorphism f between manifolds of the same dimension. This is a simple example of the concept of a natural bundle. The fact that exterior derivative d transforms sections of A kT* M into sections of A k+1T* M for every manifold M can be expressed by saying that d is an operator from A kT* M into A k+1T* M.},
    isbn = {978-3-540-56235-1},
    langid = {english},
    pagetotal = {440}
}

@ARTICLE{ssd,
    author = {{Liu}, Wei and {Anguelov}, Dragomir and {Erhan}, Dumitru and {Szegedy}, Christian and {Reed}, Scott and {Fu}, Cheng-Yang and {Berg}, Alexander C.},
    title = "{SSD: Single Shot MultiBox Detector}",
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    year = 2015,
    month = dec,
    archivePrefix = {arXiv},
    eprint = {1512.02325},
    eprinttype = {arxiv},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151202325L},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Cordts2016Cityscapes,
    title={The Cityscapes Dataset for Semantic Urban Scene Understanding},
    author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016}
}

@article{finzi2021,
    title = {A {{Practical Method}} for {{Constructing Equivariant Multilayer Perceptrons}} for {{Arbitrary Matrix Groups}}},
    author = {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
    year = {2021},
    month = apr,
    archiveprefix = {arXiv},
    eprint = {2104.09459},
    eprinttype = {arxiv},
    keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning}
}

@misc{albinLinearAnalysisManifolds,
    title = {Linear {{Analysis}} on {{Manifolds}}},
    author = {Albin, Pierre},
    langid = {english},
    howpublished = {\url{https://faculty.math.illinois.edu/~palbin/AnalysisOnMfds/LectureNotes.pdf}},
    year = {2017}
}

@article{toft2021azimuthal,
    title={Azimuthal rotational equivariance in spherical CNNs},
    author={Toft, C. and Bökman, G. and Kahl, F.},
    url={https://openreview.net/forum?id=sp3Z1jiS2vn},
    year= {2021},
    bookbitle ={ICML},
}

@book{folland2016course,
    title={A course in abstract harmonic analysis},
    author={Folland, G. B.},
    year={2016},
    publisher={CRC press}
}

@inproceedings{marcos2017,
    title = {Rotation {{Equivariant Vector Field Networks}}},
    booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
    author = {Marcos, Diego and Volpi, Michele and Komodakis, Nikos and Tuia, Devis},
    year = {2017},
    month = oct,
    pages = {5058--5067},
    issn = {2380-7504},
    doi = {10.1109/ICCV.2017.540}
}

@book{fuhr2005abstract,
    title={Abstract harmonic analysis of continuous wavelet transforms},
    author={F{\"u}hr, H.},
    year={2005},
    publisher={Springer Science \& Business Media}
}

@inproceedings{hoogeboom2018hexaconv,
    title = {{{HexaConv}}},
    booktitle = {International Conference on Learning Representations},
    author = {Hoogeboom, Emiel and Peters, Jorn W.T. and Cohen, Taco S. and Welling, Max},
    year = {2018},
    archiveprefix = {arXiv},
    eprint = {1803.02108},
    eprinttype = {arxiv},
}

@inproceedings{dieleman2016,
    title = {Exploiting {{Cyclic Symmetry}} in {{Convolutional Neural Networks}}},
    booktitle = {International {{Conference}} on {{Machine Learning}}},
    author = {Dieleman, Sander and Fauw, Jeffrey De and Kavukcuoglu, Koray},
    year = {2016},
    pages = {1889--1898},
    publisher = {{PMLR}},
    issn = {1938-7228},
    archiveprefix = {arXiv},
    eprint = {1602.02660},
    eprinttype = {arxiv},
    eventtitle = {International {{Conference}} on {{Machine Learning}}},
    langid = {english}
}

@inproceedings{bekkers2018,
    title = {Roto-{{Translation Covariant Convolutional Networks}} for {{Medical Image Analysis}}},
    booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2018},
    author = {Bekkers, Erik J. and Lafarge, Maxime W. and Veta, Mitko and Eppenhof, Koen A. J. and Pluim, Josien P. W. and Duits, Remco},
    editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-L\'opez, Carlos and Fichtinger, Gabor},
    year = {2018},
    pages = {440--448},
    publisher = {{Springer International Publishing}},
    location = {{Cham}},
    doi = {10.1007/978-3-030-00928-1_50},
    isbn = {978-3-030-00928-1},
    keywords = {Cell boundary segmentation,Group convolutional network,Mitosis detection,Roto-translation group,Vessel segmentation},
    langid = {english},
    series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{weiler2018a,
    title = {Learning {{Steerable Filters}} for {{Rotation Equivariant CNNs}}},
    booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
    author = {Weiler, Maurice and Hamprecht, Fred A. and Storath, Martin},
    year = {2018},
    pages = {849--858},
    archiveprefix = {arXiv},
    eprint = {1711.07289},
    eprinttype = {arxiv},
    eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{marshGaugeTheoriesFiber2019a,
    title = {Gauge {{Theories}} and {{Fiber Bundles}}: {{Definitions}}, {{Pictures}}, and {{Results}}},
    shorttitle = {Gauge {{Theories}} and {{Fiber Bundles}}},
    author = {Marsh, Adam},
    date = {2019-02-27},
    year = {2019},
    abstract = {A pedagogical but concise overview of fiber bundles and their connections is provided, in the context of gauge theories in physics. The emphasis is on defining and visualizing concepts and relationships between them, as well as listing common confusions, alternative notations and jargon, and relevant facts and theorems. Special attention is given to detailed figures and geometric viewpoints, some of which would seem to be novel to the literature. Topics are avoided which are well covered in textbooks, such as historical motivations, proofs and derivations, and tools for practical calculations. The present paper is best read in conjunction with the similar paper on Riemannian geometry cited herein.},
    archiveprefix = {arXiv},
    eprint = {1607.03089},
    eprinttype = {arxiv},
    keywords = {53-01 (Primary); 53-01; 55R10; 81T13; 53C07; 53Z05 (Secondary),General Relativity and Quantum Cosmology,High Energy Physics - Theory,Mathematical Physics,Mathematics - Differential Geometry}
}


@article{Gluch2021,
    title = {Noether: The More Things Change, the More Stay the Same},
    author = {Głuch, Grzegorz and Urbanke, Rüdiger},
    year = {2021},
    month = {April},
    archiveprefix = {arXiv},
    eprint = {2104.05508},
    eprinttype = {arxiv}
}

@inproceedings{Greydanus2019,
    title = {Hamiltonian Neural Networks},
    booktitle = {Proceedings of the Conference on Neural Information Processing Systems - NeurIPS 2019},
    author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
    year = {2019},
    month = {June},
    archiveprefix = {arXiv},
    eprint = {1906.01563},
    eprinttype = {arxiv}
}

@inproceedings{Toth2019,
    title = {Hamiltonian Generative Networks},
    author = {Peter Toth and Danilo Jimenez Rezende and Andrew Jaegle and Sébastien Racanière and Aleksandar Botev and Irina Higgins},
    booktitle={International Conference on Learning Representations},
    year={2020},
    archiveprefix = {arXiv},
    eprint = {1909.13789},
    eprinttype = {arxiv}
}

@inproceedings{Cranmer2020,
    title = {Lagrangian Neural Networks},
    booktitle = {Proceedings of the International Conference on Learning Representations - ICLR 2020},
    author = {Miles Cranmer and Sam Greydanus and Stephan Hoyer and Peter Battaglia and David Spergel and Shirley Ho},
    year = {2020},
    month = {March},
    archiveprefix = {arXiv},
    eprint = {2003.04630},
    eprinttype = {arxiv}
}

@article{satorras2021a,
    title = {E(n) {{Equivariant Normalizing Flows}} for {{Molecule Generation}} in {{3D}}},
    author = {Satorras, Victor Garcia and Hoogeboom, Emiel and Fuchs, Fabian B. and Posner, Ingmar and Welling, Max},
    archiveprefix = {arXiv},
    eprint = {2105.09016},
    eprinttype = {arxiv},
    year = {2021},
    keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning}
}

@book{kaniuth2013induced,
    title={Induced representations of locally compact groups},
    author={Kaniuth, E. and Taylor, K. F.},
    year={2013},
    publisher={Cambridge University Press}
}


@book{hamiltonMathematicalGaugeTheory2017,
    title = {Mathematical {{Gauge Theory}}: {{With Applications}} to the {{Standard Model}} of {{Particle Physics}}},
    shorttitle = {Mathematical {{Gauge Theory}}},
    author = {Hamilton, Mark},
    year = {2017},
    publisher = {{Springer International Publishing}},
    doi = {10.1007/978-3-319-68439-0},
    urldate = {2021-05-21},
    abstract = {The Standard Model is the foundation of modern particle and high energy physics. This book explains the mathematical background behind the Standard Model, translating ideas from physics into a mathematical language and vice versa. The first part of the book covers the mathematical theory of Lie groups and Lie algebras, fibre bundles, connections, curvature and spinors. The second part then gives a detailed exposition of how these concepts are applied in physics, concerning topics such as the Lagrangians of gauge and matter fields, spontaneous symmetry breaking, the Higgs boson and mass generation of gauge bosons and fermions. The book also contains a chapter on advanced and modern topics in particle physics, such as neutrino masses, CP violation and Grand Unification. This carefully written textbook is aimed at graduate students of mathematics and physics. It contains numerous examples and more than 150 exercises, making it suitable for self-study and use alongside lecture courses. Only a basic knowledge of differentiable manifolds and special relativity is required, summarized in the appendix.},
    isbn = {978-3-319-68438-3},
    langid = {english},
    series = {Universitext}
}


@inproceedings{melziGFramesGradientBasedLocal2019,
    ids = {melzi2019a},
    title = {{{GFrames}}: {{Gradient}}-{{Based Local Reference Frame}} for {{3D Shape Matching}}},
    shorttitle = {{{GFrames}}},
    booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    author = {Melzi, Simone and Spezialetti, Riccardo and Tombari, Federico and Bronstein, Michael M. and Stefano, Luigi Di and Rodola, Emanuele},
    year = {2019},
    pages = {4629--4638},
    issn = {2575-7075},
    doi = {10.1109/CVPR.2019.00476},
    keywords = {3D from Multiview and Sensors,Categorization,Low-level Vision,Motion and Tracking,Recognition: Detection,Retrieval}
}

@ARTICLE{2021arXiv210210333E,
    author = {{Elesedy}, Bryn and {Zaidi}, Sheheryar},
    title = "{Provably Strict Generalisation Benefit for Equivariant Models}",
    keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
    year = 2021,
    month = feb,
    archivePrefix = {arXiv},
    eprint = {2102.10333},
    eprinttype = {arxiv}
}

@misc{Howard1994,
    author        = {Ralph Howard},
    title         = {Analysis on Homogeneous Spaces},
    year          = {1994},
    howpublished  = {\url{https://people.math.sc.edu/howard/Notes/harmonic.pdf}},
    note = "Accessed May 27, 2021."
}

@book{sors2004integral,
    title={Integral geometry and geometric probability},
    author={Santal{\'o}, L.},
    year={2004},
    publisher={Cambridge university press}
}

@article{lecun1998a,
    title = {Gradient-{B}ased {L}earning {A}pplied to {D}ocument {R}ecognition},
    author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    year = {1998},
    month = nov,
    journal = {Proceedings of the IEEE},
    volume = {86},
    number = {11},
    pages = {2278--2324},
    issn = {1558-2256},
    doi = {10.1109/5.726791},
    keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis}
}

@article{xiao2017,
    title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
    shorttitle = {Fashion-{{MNIST}}},
    author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
    year = {2017},
    month = sep,
    journal = {arXiv:1708.07747 [cs, stat]},
    eprint = {1708.07747},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}
@article{arnaudoContrastiveDistillationApproach2021,
    title = {A {{Contrastive Distillation Approach}} for {{Incremental Semantic Segmentation}} in {{Aerial Images}}},
    author = {Arnaudo, Edoardo and Cermelli, Fabio and Tavera, Antonio and Rossi, Claudio and Caputo, Barbara},
    date = {2021-12-07},
    year = {2021},
    eprint = {2112.03814},
    eprinttype = {arxiv},
    primaryclass = {cs, eess},
    url = {http://arxiv.org/abs/2112.03814},
    urldate = {2022-01-11},
    abstract = {Incremental learning represents a crucial task in aerial image processing, especially given the limited availability of large-scale annotated datasets. A major issue concerning current deep neural architectures is known as catastrophic forgetting, namely the inability to faithfully maintain past knowledge once a new set of data is provided for retraining. Over the years, several techniques have been proposed to mitigate this problem for image classification and object detection. However, only recently the focus has shifted towards more complex downstream tasks such as instance or semantic segmentation. Starting from incremental-class learning for semantic segmentation tasks, our goal is to adapt this strategy to the aerial domain, exploiting a peculiar feature that differentiates it from natural images, namely the orientation. In addition to the standard knowledge distillation approach, we propose a contrastive regularization, where any given input is compared with its augmented version (i.e. flipping and rotations) in order to minimize the difference between the segmentation features produced by both inputs. We show the effectiveness of our solution on the Potsdam dataset, outperforming the incremental baseline in every test. Code available at: https://github.com/edornd/contrastive-distillation.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
    file = {/home/oscar/Zotero/storage/T8MPIH5T/Arnaudo et al. - 2021 - A Contrastive Distillation Approach for Incrementa.pdf;/home/oscar/Zotero/storage/UCBFM6VR/2112.html}
}

@article{banerjeeVolterraNetHigherOrder2021,
    title = {{{VolterraNet}}: {{A}} Higher Order Convolutional Network with Group Equivariance for Homogeneous Manifolds},
    shorttitle = {{{VolterraNet}}},
    author = {Banerjee, Monami and Chakraborty, Rudrasis and Bouza, Jose and Vemuri, Baba C.},
    date = {2021-06-05},
    year = {2021},
    url = {https://arxiv.org/abs/2106.15301v1},
    urldate = {2022-01-11},
    abstract = {Convolutional neural networks have been highly successful in image-based learning tasks due to their translation equivariance property. Recent work has generalized the traditional convolutional layer of a convolutional neural network to non-Euclidean spaces and shown group equivariance of the generalized convolution operation. In this paper, we present a novel higher order Volterra convolutional neural network (VolterraNet) for data defined as samples of functions on Riemannian homogeneous spaces. Analagous to the result for traditional convolutions, we prove that the Volterra functional convolutions are equivariant to the action of the isometry group admitted by the Riemannian homogeneous spaces, and under some restrictions, any non-linear equivariant function can be expressed as our homogeneous space Volterra convolution, generalizing the non-linear shift equivariant characterization of Volterra expansions in Euclidean space. We also prove that second order functional convolution operations can be represented as cascaded convolutions which leads to an efficient implementation. Beyond this, we also propose a dilated VolterraNet model. These advances lead to large parameter reductions relative to baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet performance, we present several real data experiments involving classification tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing on diffusion MRI data. Performance comparisons to the state-of-the-art are also presented.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/ANPLQRDR/Banerjee et al. - 2021 - VolterraNet A higher order convolutional network .pdf;/home/oscar/Zotero/storage/TJYD9433/2106.html}
}

@article{bentonLearningInvariancesNeural2020,
    title = {Learning {{Invariances}} in {{Neural Networks}}},
    author = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
    date = {2020-12-01},
    year = {2020},
    eprint = {2010.11882},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    url = {http://arxiv.org/abs/2010.11882},
    urldate = {2022-01-11},
    abstract = {Invariances to translations have imbued convolutional neural networks with powerful generalization properties. However, we often do not know a priori what invariances are present in the data, or to what extent a model should be invariant to a given symmetry group. We show how to \textbackslash emph\{learn\} invariances and equivariances by parameterizing a distribution over augmentations and optimizing the training loss simultaneously with respect to the network parameters and augmentation parameters. With this simple procedure we can recover the correct set and extent of invariances on image classification, regression, segmentation, and molecular property prediction from a large space of augmentations, on training data alone.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/RWJR6F5B/Benton et al. - 2020 - Learning Invariances in Neural Networks.pdf}
}

@inproceedings{boscainiLearningShapeCorrespondence2016,
    title = {Learning Shape Correspondence with Anisotropic Convolutional Neural Networks},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Bronstein, Michael},
    date = {2016},
    year = {2016},
    volume = {29},
    publisher = {{Curran Associates, Inc.}},
    url = {https://proceedings.neurips.cc/paper/2016/hash/228499b55310264a8ea0e27b6e7c6ab6-Abstract.html},
    urldate = {2022-01-13},
    file = {/home/oscar/Zotero/storage/PT48C3AL/Boscaini et al. - 2016 - Learning shape correspondence with anisotropic con.pdf}
}

@article{chaitanyaLocalContrastiveLoss2021,
    title = {Local Contrastive Loss with Pseudo-Label Based Self-Training for Semi-Supervised Medical Image Segmentation},
    author = {Chaitanya, Krishna and Erdil, Ertunc and Karani, Neerav and Konukoglu, Ender},
    date = {2021-12-17},
    year = {2021},
    eprint = {2112.09645},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    url = {http://arxiv.org/abs/2112.09645},
    urldate = {2022-01-11},
    abstract = {Supervised deep learning-based methods yield accurate results for medical image segmentation. However, they require large labeled datasets for this, and obtaining them is a laborious task that requires clinical expertise. Semi/self-supervised learning-based approaches address this limitation by exploiting unlabeled data along with limited annotated data. Recent self-supervised learning methods use contrastive loss to learn good global level representations from unlabeled images and achieve high performance in classification tasks on popular natural image datasets like ImageNet. In pixel-level prediction tasks such as segmentation, it is crucial to also learn good local level representations along with global representations to achieve better accuracy. However, the impact of the existing local contrastive loss-based methods remains limited for learning good local representations because similar and dissimilar local regions are defined based on random augmentations and spatial proximity; not based on the semantic label of local regions due to lack of large-scale expert annotations in the semi/self-supervised setting. In this paper, we propose a local contrastive loss to learn good pixel level features useful for segmentation by exploiting semantic label information obtained from pseudo-labels of unlabeled images alongside limited annotated images. In particular, we define the proposed loss to encourage similar representations for the pixels that have the same pseudo-label/ label while being dissimilar to the representation of pixels with different pseudo-label/label in the dataset. We perform pseudo-label based self-training and train the network by jointly optimizing the proposed contrastive loss on both labeled and unlabeled sets and segmentation loss on only the limited labeled set. We evaluated on three public cardiac and prostate datasets, and obtain high segmentation performance.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/G7HF7IPH/Chaitanya et al. - 2021 - Local contrastive loss with pseudo-label based sel.pdf;/home/oscar/Zotero/storage/D9QFMLV6/2112.html}
}

@article{chakrabortyCNNHomogneousRiemannian2018,
    title = {A {{CNN}} for Homogneous {{Riemannian}} Manifolds with Applications to {{Neuroimaging}}},
    author = {Chakraborty, Rudrasis and Banerjee, Monami and Vemuri, Baba C.},
    date = {2018-05-14},
    year = {2018},
    url = {https://arxiv.org/abs/1805.05487v3},
    urldate = {2022-01-11},
    abstract = {Convolutional neural networks are ubiquitous in Machine Learning applications for solving a variety of problems. They however can not be used in their native form when the domain of the data is commonly encountered manifolds such as the sphere, the special orthogonal group, the Grassmanian, the manifold of symmetric positive definite matrices and others. Most recently, generalization of CNNs to data domains such as the 2-sphere has been reported by some research groups, which is referred to as the spherical CNNs (SCNNs). The key property of SCNNs distinct from CNNs is that they exhibit the rotational equivariance property that allows for sharing learned weights within a layer. In this paper, we theoretically generalize the CNNs to Riemannian homogeneous manifolds, that include but are not limited to the aforementioned example manifolds. Our key contributions in this work are: (i) A theorem stating that linear group equivariance systems are fully characterized by correlation of functions on the domain manifold and vice-versa. This is fundamental to the characterization of all linear group equivariant systems and parallels the widely used result in linear system theory for vector spaces. (ii) As a corrolary, we prove the equivariance of the correlation operation to group actions admitted by the input domains which are Riemannian homogeneous manifolds. (iii) We present the first end-to-end deep network architecture for classification of diffusion magnetic resonance image (dMRI) scans acquired from a cohort of 44 Parkinson Disease patients and 50 control/normal subjects. (iv) A proof of concept experiment involving synthetic data generated on the manifold of symmetric positive definite matrices is presented to demonstrate the applicability of our network to other types of domains.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/SDPAMP72/Chakraborty et al. - 2018 - A CNN for homogneous Riemannian manifolds with app.pdf;/home/oscar/Zotero/storage/YP7QLL2E/1805.html}
}

@article{chenLearningSemanticSegmentation2019,
    title = {Learning {{Semantic Segmentation}} from {{Synthetic Data}}: {{A Geometrically Guided Input-Output Adaptation Approach}}},
    shorttitle = {Learning {{Semantic Segmentation}} from {{Synthetic Data}}},
    author = {Chen, Yuhua and Li, Wen and Chen, Xiaoran and Van Gool, Luc},
    date = {2019-01-13},
    year = {2019},
    eprint = {1812.05040},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/1812.05040},
    urldate = {2022-01-11},
    abstract = {Recently, increasing attention has been drawn to training semantic segmentation models using synthetic data and computer-generated annotation. However, domain gap remains a major barrier and prevents models learned from synthetic data from generalizing well to real-world applications. In this work, we take the advantage of additional geometric information from synthetic data, a powerful yet largely neglected cue, to bridge the domain gap. Such geometric information can be generated easily from synthetic data, and is proven to be closely coupled with semantic information. With the geometric information, we propose a model to reduce domain shift on two levels: on the input level, we augment the traditional image translation network with the additional geometric information to translate synthetic images into realistic styles; on the output level, we build a task network which simultaneously performs depth estimation and semantic segmentation on the synthetic data. Meanwhile, we encourage the network to preserve correlation between depth and semantics by adversarial training on the output space. We then validate our method on two pairs of synthetic to real dataset: Virtual KITTI to KITTI, and SYNTHIA to Cityscapes, where we achieve a significant performance gain compared to the non-adapt baseline and methods using only semantic label. This demonstrates the usefulness of geometric information from synthetic data for cross-domain semantic segmentation.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/4YYS4RH6/Chen et al. - 2019 - Learning Semantic Segmentation from Synthetic Data.pdf;/home/oscar/Zotero/storage/U8ITKRNH/1812.html}
}

@article{chenMaskbasedDataAugmentation2021,
    title = {Mask-Based {{Data Augmentation}} for {{Semi-supervised Semantic Segmentation}}},
    author = {Chen, Ying and Ouyang, Xu and Zhu, Kaiyue and Agam, Gady},
    date = {2021-01-25},
    year = {2021},
    eprint = {2101.10156},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2101.10156},
    urldate = {2022-01-11},
    abstract = {Semantic segmentation using convolutional neural networks (CNN) is a crucial component in image analysis. Training a CNN to perform semantic segmentation requires a large amount of labeled data, where the production of such labeled data is both costly and labor intensive. Semi-supervised learning algorithms address this issue by utilizing unlabeled data and so reduce the amount of labeled data needed for training. In particular, data augmentation techniques such as CutMix and ClassMix generate additional training data from existing labeled data. In this paper we propose a new approach for data augmentation, termed ComplexMix, which incorporates aspects of CutMix and ClassMix with improved performance. The proposed approach has the ability to control the complexity of the augmented data while attempting to be semantically-correct and address the tradeoff between complexity and correctness. The proposed ComplexMix approach is evaluated on a standard dataset for semantic segmentation and compared to other state-of-the-art techniques. Experimental results show that our method yields improvement over state-of-the-art methods on standard datasets for semantic image segmentation.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/RJLACMLI/Chen et al. - 2021 - Mask-based Data Augmentation for Semi-supervised S.pdf;/home/oscar/Zotero/storage/W28U4RS9/2101.html}
}

@article{cohenConvolutionalNetworksSpherical2017a,
    title = {Convolutional {{Networks}} for {{Spherical Signals}}},
    author = {Cohen, Taco and Geiger, Mario and Köhler, Jonas and Welling, Max},
    date = {2017-09-14},
    year = {2017},
    url = {https://arxiv.org/abs/1709.04893v2},
    urldate = {2022-01-11},
    abstract = {The success of convolutional networks in learning problems involving planar signals such as images is due to their ability to exploit the translation symmetry of the data distribution through weight sharing. Many areas of science and egineering deal with signals with other symmetries, such as rotation invariant data on the sphere. Examples include climate and weather science, astrophysics, and chemistry. In this paper we present spherical convolutional networks. These networks use convolutions on the sphere and rotation group, which results in rotational weight sharing and rotation equivariance. Using a synthetic spherical MNIST dataset, we show that spherical convolutional networks are very effective at dealing with rotationally invariant classification problems.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/39Z3M32M/Cohen et al. - 2017 - Convolutional Networks for Spherical Signals.pdf;/home/oscar/Zotero/storage/QW2B8X9C/1709.html}
}

@incollection{coorsSphereNetLearningSpherical2018,
    title = {{{SphereNet}}: {{Learning Spherical Representations}} for {{Detection}} and {{Classification}} in {{Omnidirectional Images}}},
    shorttitle = {{{SphereNet}}},
    booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
    author = {Coors, Benjamin and Condurache, Alexandru Paul and Geiger, Andreas},
    editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
    date = {2018},
    year = {2018},
    series = {Lecture {{Notes}} in {{Computer Science}}},
    volume = {11213},
    pages = {525--541},
    publisher = {{Springer International Publishing}},
    location = {{Cham}},
    doi = {10.1007/978-3-030-01240-3_32},
    url = {http://link.springer.com/10.1007/978-3-030-01240-3_32},
    urldate = {2022-01-13},
    abstract = {Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets.},
    isbn = {978-3-030-01239-7 978-3-030-01240-3},
    langid = {english},
    file = {/home/oscar/Zotero/storage/D62UJRM6/Coors et al. - 2018 - SphereNet Learning Spherical Representations for .pdf}
}


@inproceedings{daiDeformableConvolutionalNetworks2017,
    title = {Deformable {{Convolutional Networks}}},
    booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
    author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
    date = {2017-10},
    year = {2017},
    pages = {764--773},
    issn = {2380-7504},
    doi = {10.1109/ICCV.2017.89},
    abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.},
    eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
    keywords = {Convolution,Feature extraction,Kernel,Object detection,Standards,Two dimensional displays},
    file = {/home/oscar/Zotero/storage/6NUPVQ33/Dai et al. - 2017 - Deformable Convolutional Networks.pdf;/home/oscar/Zotero/storage/X3MNERGU/8237351.html}
}

@article{defferrardDeepSphereGraphbasedSpherical2020,
    title = {{{DeepSphere}}: A Graph-Based Spherical {{CNN}}},
    shorttitle = {{{DeepSphere}}},
    author = {Defferrard, Michaël and Milani, Martino and Gusset, Frédérick and Perraudin, Nathanaël},
    date = {2020-12-30},
    year = {2020},
    url = {https://arxiv.org/abs/2012.15000v1},
    urldate = {2022-01-11},
    abstract = {Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the sampled sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https://github.com/deepsphere},
    langid = {english},
    file = {/home/oscar/Zotero/storage/BYUZQQU9/Defferrard et al. - 2020 - DeepSphere a graph-based spherical CNN.pdf;/home/oscar/Zotero/storage/UEZTRKK4/2012.html}
}

@article{deyGroupEquivariantGenerative2021,
    title = {Group {{Equivariant Generative Adversarial Networks}}},
    author = {Dey, Neel and Chen, Antong and Ghafurian, Soheil},
    date = {2021-03-30},
    year = {2021},
    eprint = {2005.01683},
    eprinttype = {arxiv},
    primaryclass = {cs, eess},
    url = {http://arxiv.org/abs/2005.01683},
    urldate = {2022-01-11},
    abstract = {Recent improvements in generative adversarial visual synthesis incorporate real and fake image transformation in a self-supervised setting, leading to increased stability and perceptual fidelity. However, these approaches typically involve image augmentations via additional regularizers in the GAN objective and thus spend valuable network capacity towards approximating transformation equivariance instead of their desired task. In this work, we explicitly incorporate inductive symmetry priors into the network architectures via group-equivariant convolutional networks. Group-convolutions have higher expressive power with fewer samples and lead to better gradient feedback between generator and discriminator. We show that group-equivariance integrates seamlessly with recent techniques for GAN training across regularizers, architectures, and loss functions. We demonstrate the utility of our methods for conditional synthesis by improving generation in the limited data regime across symmetric imaging datasets and even find benefits for natural images with preferred orientation.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
    file = {/home/oscar/Zotero/storage/VE87AG7T/Dey et al. - 2021 - Group Equivariant Generative Adversarial Networks.pdf}
}

@article{duSphericalTransformerAdapting2021,
    title = {Spherical {{Transformer}}: {{Adapting Spherical Signal}} to {{CNNs}}},
    shorttitle = {Spherical {{Transformer}}},
    author = {Du, Haikuan and Cao, Hui and Cai, Shen and Yan, Junchi and Zhang, Siyu},
    date = {2021-01-24},
    year = {2021},
    month = jan,
    eprint = {2101.03848},
    journal = {arXiv:2101.03848 [cs]},
    eprinttype = {arxiv},
    primaryclass = {cs},
    urldate = {2022-01-11},
    abstract = {Convolutional neural networks (CNNs) have been widely used in various vision tasks, e.g. image classification, semantic segmentation, etc. Unfortunately, standard 2D CNNs are not well suited for spherical signals such as panorama images or spherical projections, as the sphere is an unstructured grid. In this paper, we present Spherical Transformer which can transform spherical signals into vectors that can be directly processed by standard CNNs such that many well-designed CNNs architectures can be reused across tasks and datasets by pretraining. To this end, the proposed method first uses locally structured sampling methods such as HEALPix to construct a transformer grid by using the information of spherical points and its adjacent points, and then transforms the spherical signals to the vectors through the grid. By building the Spherical Transformer module, we can use multiple CNN architectures directly. We evaluate our approach on the tasks of spherical MNIST recognition, 3D object classification and omnidirectional image semantic segmentation. For 3D object classification, we further propose a rendering-based projection method to improve the performance and a rotational-equivariant model to improve the anti-rotation ability. Experimental results on three tasks show that our approach achieves superior performance over state-of-the-art methods.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/TMR2F7LW/Du et al. - 2021 - Spherical Transformer Adapting Spherical Signal t.pdf;/home/oscar/Zotero/storage/9H3HAC2U/2101.html}
}

@inproceedings{eder2019convolutions,
    title={Convolutions on {S}pherical {I}mages},
    author={Eder, Marc and Frahm, Jan-Michael},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
    pages={1--5},
    year={2019},
    month = jun
}

@article{ederConvolutionsSphericalImages2019a,
    title = {Convolutions on {{Spherical Images}}},
    author = {Eder, Marc and Frahm, Jan-Michael},
    date = {2019-05-20},
    year = {2019},
    eprint = {1905.08409},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/1905.08409},
    urldate = {2022-01-11},
    abstract = {Applying convolutional neural networks to spherical images requires particular considerations. We look to the millennia of work on cartographic map projections to provide the tools to define an optimal representation of spherical images for the convolution operation. We propose a representation for deep spherical image inference based on the icosahedral Snyder equal-area (ISEA) projection, a projection onto a geodesic grid, and show that it vastly exceeds the state-of-the-art for convolution on spherical images, improving semantic segmentation results by 12.6\%.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/Z42RMDM8/Eder and Frahm - 2019 - Convolutions on Spherical Images.pdf;/home/oscar/Zotero/storage/B27C2IBZ/1905.html}
}

@article{ederMappedConvolutions2019,
    title = {Mapped {{Convolutions}}},
    author = {Eder, Marc and Price, True and Vu, Thanh and Bapat, Akash and Frahm, Jan-Michael},
    date = {2019-06-26},
    year = {2019},
    eprint = {1906.11096},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/1906.11096},
    urldate = {2022-01-13},
    abstract = {We present a versatile formulation of the convolution operation that we term a "mapped convolution." The standard convolution operation implicitly samples the pixel grid and computes a weighted sum. Our mapped convolution decouples these two components, freeing the operation from the confines of the image grid and allowing the kernel to process any type of structured data. As a test case, we demonstrate its use by applying it to dense inference on spherical data. We perform an in-depth study of existing spherical image convolution methods and propose an improved sampling method for equirectangular images. Then, we discuss the impact of data discretization when deriving a sampling function, highlighting drawbacks of the cube map representation for spherical data. Finally, we illustrate how mapped convolutions enable us to convolve directly on a mesh by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method exceeds the state of the art for spherical depth estimation by nearly 17\%. Our findings suggest that mapped convolutions can be instrumental in expanding the application scope of convolutional neural networks.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/GGCXGR7V/Eder et al. - 2019 - Mapped Convolutions.pdf;/home/oscar/Zotero/storage/623BUKGQ/1906.html}
}

@article{ederTangentImagesMitigating2019,
    title = {Tangent {{Images}} for {{Mitigating Spherical Distortion}}},
    author = {Eder, Marc and Shvets, Mykhailo and Lim, John and Frahm, Jan-Michael},
    date = {2019-12-19},
    year = {2019},
    url = {https://arxiv.org/abs/1912.09390v3},
    urldate = {2022-01-11},
    abstract = {In this work, we propose "tangent images," a spherical image representation that facilitates transferable and scalable \$360\^\textbackslash circ\$ computer vision. Inspired by techniques in cartography and computer graphics, we render a spherical image to a set of distortion-mitigated, locally-planar image grids tangent to a subdivided icosahedron. By varying the resolution of these grids independently of the subdivision level, we can effectively represent high resolution spherical images while still benefiting from the low-distortion icosahedral spherical approximation. We show that training standard convolutional neural networks on tangent images compares favorably to the many specialized spherical convolutional kernels that have been developed, while also scaling efficiently to handle significantly higher spherical resolutions. Furthermore, because our approach does not require specialized kernels, we show that we can transfer networks trained on perspective images to spherical data without fine-tuning and with limited performance drop-off. Finally, we demonstrate that tangent images can be used to improve the quality of sparse feature detection on spherical images, illustrating its usefulness for traditional computer vision tasks like structure-from-motion and SLAM.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/ME2C7669/Eder et al. - 2019 - Tangent Images for Mitigating Spherical Distortion.pdf;/home/oscar/Zotero/storage/XPPQZ5CR/1912.html}
}

@inproceedings{estevesLearningEquivariantRepresentations2018a,
    title = {Learning {{SO}}(3) {{Equivariant Representations}} with {{Spherical CNNs}}},
    booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
    author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
    editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
    date = {2018},
    year = {2018},
    series = {Lecture {{Notes}} in {{Computer Science}}},
    pages = {54--70},
    publisher = {{Springer International Publishing}},
    location = {{Cham}},
    doi = {10.1007/978-3-030-01261-8_4},
    abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
    isbn = {978-3-030-01261-8},
    langid = {english},
    keywords = {Convolutional Neural Network (CNNs),Group Convolution,Spherical Convolution,Spherical Fourier Transform (SFT),Spherical Harmonic Domain},
    file = {/home/oscar/Zotero/storage/SLDMJ5JT/Esteves et al. - 2018 - Learning SO(3) Equivariant Representations with Sp.pdf}
}

@article{fanRevisitingConsistencyRegularization2021,
    title = {Revisiting {{Consistency Regularization}} for {{Semi-Supervised Learning}}},
    author = {Fan, Yue and Kukleva, Anna and Schiele, Bernt},
    date = {2021-12-10},
    year = {2021},
    eprint = {2112.05825},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2112.05825},
    urldate = {2022-01-11},
    abstract = {Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art for various datasets and settings and outperforms previous work by a significant margin, particularly in low data regimes. Extensive experiments are conducted to analyze the method, and the code will be published.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/MHDI738A/Fan et al. - 2021 - Revisiting Consistency Regularization for Semi-Sup.pdf;/home/oscar/Zotero/storage/NBQGR6TV/2112.html}
}

@article{gagliardiHomogeneousSphericalData2013,
    title = {Homogeneous Spherical Data of Orbits in Spherical Embeddings},
    author = {Gagliardi, Giuliano and Hofscheier, Johannes},
    date = {2013-12-10},
    year = {2013},
    doi = {10.1007/s00031-014-9297-2},
    url = {https://arxiv.org/abs/1312.2940v2},
    urldate = {2022-01-11},
    abstract = {Let \$G\$ be a connected reductive complex algebraic group. Luna assigned to any spherical homogeneous space \$G/H\$ a combinatorial object called a homogeneous spherical datum. By a theorem of Losev, this object uniquely determines \$G/H\$ up to \$G\$-equivariant isomorphism. In this paper, we determine the homogeneous spherical datum of a \$G\$-orbit \$X\_0\$ in a spherical embedding \$G/H \textbackslash hookrightarrow X\$. As an application, we obtain a description of the colored fan associated to the spherical embedding \$X\_0 \textbackslash hookrightarrow \textbackslash bar\{X\_0\}\$.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/PLAUNRL5/Gagliardi and Hofscheier - 2013 - Homogeneous spherical data of orbits in spherical .pdf;/home/oscar/Zotero/storage/H48K54WV/1312.html}
}

@article{gandikotaTrainingArchitectureHow2021,
    title = {Training or {{Architecture}}? {{How}} to {{Incorporate Invariance}} in {{Neural Networks}}},
    shorttitle = {Training or {{Architecture}}?},
    author = {Gandikota, Kanchana Vaishnavi and Geiping, Jonas and Lähner, Zorah and Czapliński, Adam and Moeller, Michael},
    date = {2021-06-18},
    year = {2021},
    eprint = {2106.10044},
    month = jun,
    eprinttype = {arxiv},
    primaryclass = {cs},
    journal = {arXiv:2106.10044 [cs]},
    urldate = {2022-01-11},
    abstract = {Many applications require the robustness, or ideally the invariance, of a neural network to certain transformations of input data. Most commonly, this requirement is addressed by either augmenting the training data, using adversarial training, or defining network architectures that include the desired invariance automatically. Unfortunately, the latter often relies on the ability to enlist all possible transformations, which make such approaches largely infeasible for infinite sets of transformations, such as arbitrary rotations or scaling. In this work, we propose a method for provably invariant network architectures with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. We analyze properties of such approaches, extend them to equivariant networks, and demonstrate their advantages in terms of robustness as well as computational efficiency in several numerical examples. In particular, we investigate the robustness with respect to rotations of images (which can possibly hold up to discretization artifacts only) as well as the provable rotational and scaling invariance of 3D point cloud classification.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/8CU67B47/Gandikota et al. - 2021 - Training or Architecture How to Incorporate Invar.pdf;/home/oscar/Zotero/storage/UXEE9HE7/2106.html}
}

@article{gaoSphericalConvolutionalNeural2020,
    title = {Spherical {{Convolutional Neural Networks}}: {{Stability}} to {{Perturbations}} in {{SO}}(3)},
    shorttitle = {Spherical {{Convolutional Neural Networks}}},
    author = {Gao, Zhan and Gama, Fernando and Ribeiro, Alejandro},
    date = {2020-10-12},
    year = {2020},
    url = {https://arxiv.org/abs/2010.05865v2},
    urldate = {2022-01-11},
    abstract = {Spherical convolutional neural networks (Spherical CNNs) learn nonlinear representations from 3D data by exploiting the data structure and have shown promising performance in shape analysis, object classification, and planning among others. This paper investigates the properties that Spherical CNNs exhibit as they pertain to the rotational structure inherent in spherical signals. We build upon the rotation equivariance of spherical convolutions to show that Spherical CNNs are stable to general structure perturbations. In particular, we model arbitrary structure perturbations as diffeomorphism perturbations, and define the rotation distance that measures how far from rotations these perturbations are. We prove that the output change of a Spherical CNN induced by the diffeomorphism perturbation is bounded proportionally by the perturbation size under the rotation distance. This stability property coupled with the rotation equivariance provide theoretical guarantees that underpin the practical observations that Spherical CNNs exploit the rotational structure, maintain performance under structure perturbations that are close to rotations, and offer good generalization and faster learning.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/23CJQ6HE/Gao et al. - 2020 - Spherical Convolutional Neural Networks Stability.pdf}
}

@article{goyalDatasetAugmentationSynthetic2018,
    title = {Dataset {{Augmentation}} with {{Synthetic Images Improves Semantic Segmentation}}},
    author = {Goyal, Manik and Rajpura, Param and Bojinov, Hristo and Hegde, Ravi},
    date = {2018},
    year = {2018},
    volume = {841},
    eprint = {1709.00849},
    eprinttype = {arxiv},
    primaryclass = {cs},
    pages = {348--359},
    doi = {10.1007/978-981-13-0020-2_31},
    url = {http://arxiv.org/abs/1709.00849},
    urldate = {2022-01-11},
    abstract = {Although Deep Convolutional Neural Networks trained with strong pixel-level annotations have significantly pushed the performance in semantic segmentation, annotation efforts required for the creation of training data remains a roadblock for further improvements. We show that augmentation of the weakly annotated training dataset with synthetic images minimizes both the annotation efforts and also the cost of capturing images with sufficient variety. Evaluation on the PASCAL 2012 validation dataset shows an increase in mean IOU from 52.80\% to 55.47\% by adding just 100 synthetic images per object class. Our approach is thus a promising solution to the problems of annotation and dataset collection.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/9MN58M7A/Goyal et al. - 2018 - Dataset Augmentation with Synthetic Images Improve.pdf;/home/oscar/Zotero/storage/KNNGNZ5U/1709.html}
}

@inproceedings{haim2019surface,
    title={Surface {N}etworks via {G}eneral {C}overs},
    author={Haim, Niv and Segol, Nimrod and Ben-Hamu, Heli and Maron, Haggai and Lipman, Yaron},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={632--641},
    year={2019},
    month = oct
}

@article{haimSurfaceNetworksGeneral2019,
    title = {Surface {{{N}etworks}} via {{{G}eneral {C}overs}}},
    author = {Haim, Niv and Segol, Nimrod and Ben-Hamu, Heli and Maron, Haggai and Lipman, Yaron},
    date = {2019-08-18},
    year = {2019},
    month = oct,
    eprint = {1812.10705},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/1812.10705},
    urldate = {2022-01-11},
    abstract = {Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models as well as spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/TRQ73SMP/Haim et al. - 2019 - Surface Networks via General Covers.pdf;/home/oscar/Zotero/storage/BZTKR7TG/1812.html}
}

@article{hanReDetRotationequivariantDetector2021a,
    title = {{{ReDet}}: {{A Rotation-equivariant Detector}} for {{Aerial Object Detection}}},
    shorttitle = {{{ReDet}}},
    author = {Han, Jiaming and Ding, Jian and Xue, Nan and Xia, Gui-Song},
    date = {2021-03-13},
    year = {2021},
    eprint = {2103.07733},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2103.07733},
    urldate = {2022-01-11},
    abstract = {Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefficient. Moreover, as ordinary CNNs do not explicitly model the orientation variation, large amounts of rotation augmented data is needed to train an accurate object detector. In this paper, we propose a Rotation-equivariant Detector (ReDet) to address these issues, which explicitly encodes rotation equivariance and rotation invariance. More precisely, we incorporate rotation-equivariant networks into the detector to extract rotation-equivariant features, which can accurately predict the orientation and lead to a huge reduction of model size. Based on the rotation-equivariant features, we also present Rotation-invariant RoI Align (RiRoI Align), which adaptively extracts rotation-invariant features from equivariant features according to the orientation of RoI. Extensive experiments on several challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can achieve state-of-the-art performance on the task of aerial object detection. Compared with previous best results, our ReDet gains 1.2, 3.5 and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing the number of parameters by 60\textbackslash\% (313 Mb vs. 121 Mb). The code is available at: \textbackslash url\{https://github.com/csuhan/ReDet\}.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/N9KCAIMA/Han et al. - 2021 - ReDet A Rotation-equivariant Detector for Aerial .pdf;/home/oscar/Zotero/storage/GB3L9IWN/2103.html}
}

@article{henriquesGeneratingDataAugmentation2021,
    title = {Generating {{Data Augmentation}} Samples for {{Semantic Segmentation}} of {{Salt Bodies}} in a {{Synthetic Seismic Image Dataset}}},
    author = {Henriques, Luis Felipe and Colcher, Sérgio and Milidiú, Ruy Luiz and Bulcão, André and Barros, Pablo},
    date = {2021-06-17},
    year = {2021},
    eprint = {2106.08269},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2106.08269},
    urldate = {2022-01-11},
    abstract = {Nowadays, subsurface salt body localization and delineation, also called semantic segmentation of salt bodies, are among the most challenging geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky and is crucial for identifying hydrocarbon reservoirs and drill path planning. This work proposes a Data Augmentation method based on training two generative models to augment the number of samples in a seismic image dataset for the semantic segmentation of salt bodies. Our method uses deep learning models to generate pairs of seismic image patches and their respective salt masks for the Data Augmentation. The first model is a Variational Autoencoder and is responsible for generating patches of salt body masks. The second is a Conditional Normalizing Flow model, which receives the generated masks as inputs and generates the associated seismic image patches. We evaluate the proposed method by comparing the performance of ten distinct state-of-the-art models for semantic segmentation, trained with and without the generated augmentations, in a dataset from two synthetic seismic images. The proposed methodology yields an average improvement of 8.57\% in the IoU metric across all compared models. The best result is achieved by a DeeplabV3+ model variant, which presents an IoU score of 95.17\% when trained with our augmentations. Additionally, our proposal outperformed six selected data augmentation methods, and the most significant improvement in the comparison, of 9.77\%, is achieved by composing our DA with augmentations from an elastic transformation. At last, we show that the proposed method is adaptable for a larger context size by achieving results comparable to the obtained on the smaller context size.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
    file = {/home/oscar/Zotero/storage/J7L7Q4CT/Henriques et al. - 2021 - Generating Data Augmentation samples for Semantic .pdf;/home/oscar/Zotero/storage/HQ9ECBJ4/2106.html}
}

@article{hwangEquivariancebridgedInvariantRepresentation2021a,
    title = {Equivariance-Bridged {{SO}}(2)-{{Invariant Representation Learning}} Using {{Graph Convolutional Network}}},
    author = {Hwang, Sungwon and Lim, Hyungtae and Myung, Hyun},
    date = {2021-10-20},
    year = {2021},
    eprint = {2106.09996},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2106.09996},
    urldate = {2022-01-11},
    abstract = {Training a Convolutional Neural Network (CNN) to be robust against rotation has mostly been done with data augmentation. In this paper, another progressive vision of research direction is highlighted to encourage less dependence on data augmentation by achieving structural rotational invariance of a network. The deep equivariance-bridged SO(2) invariant network is proposed to echo such vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the graph representation of an image to acquire rotationally equivariant representation, as GCN is more suitable for constructing deeper network than spectral graph convolution-based approaches. Then, invariant representation is eventually obtained with Global Average Pooling (GAP), a permutation-invariant operation suitable for aggregating high-dimensional representations, over the equivariant set of vertices retrieved from SWN-GCN. Our method achieves the state-of-the-art image classification performance on rotated MNIST and CIFAR-10 images, where the models are trained with a non-augmented dataset only. Quantitative validations over invariance of the representations also demonstrate strong invariance of deep representations of SWN-GCN over rotations.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/CUCHUWBC/Hwang et al. - 2021 - Equivariance-bridged SO(2)-Invariant Representatio.pdf;/home/oscar/Zotero/storage/JHCTDC2E/2106.html}
}

@article{jeonActiveConvolutionLearning2017,
    title = {Active {{Convolution}}: {{Learning}} the {{Shape}} of {{Convolution}} for {{Image Classification}}},
    shorttitle = {Active {{Convolution}}},
    author = {Jeon, Yunho and Kim, Junmo},
    date = {2017},
    year = {2017},
    journaltitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    doi = {10.1109/cvpr.2017.200},
    url = {https://scholar.archive.org/work/gshoq2jm5vbtrkkndohh3wd2dm},
    urldate = {2022-01-13},
    langid = {english},
    file = {/home/oscar/Zotero/storage/SIA9EABW/Jeon and Kim - 2017 - Active Convolution Learning the Shape of Convolut.pdf;/home/oscar/Zotero/storage/85H8366Y/gshoq2jm5vbtrkkndohh3wd2dm.html}
}

@article{kumarOmniDetSurroundView2021,
    title = {{{OmniDet}}: {{Surround View Cameras}} Based {{Multi-task Visual Perception Network}} for {{Autonomous Driving}}},
    shorttitle = {{{OmniDet}}},
    author = {Kumar, Varun Ravi and Yogamani, Senthil and Rashed, Hazem and Sistu, Ganesh and Witt, Christian and Leang, Isabelle and Milz, Stefan and Mäder, Patrick},
    date = {2021-08-24},
    year = {2021},
    eprint = {2102.07448},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2102.07448},
    urldate = {2022-01-14},
    abstract = {Surround View fisheye cameras are commonly deployed in automated driving for 360\textbackslash deg\{\} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
    file = {/home/oscar/Zotero/storage/VD28LVT6/Kumar et al. - 2021 - OmniDet Surround View Cameras based Multi-task Vi.pdf;/home/oscar/Zotero/storage/DCWJTKZN/2102.html}
}

@article{laiSemanticDrivenGenerationHyperlapse2018,
    title = {Semantic-{{Driven Generation}} of {{Hyperlapse}} from 360 {{Degree Video}}},
    author = {Lai, Wei-Sheng and Huang, Yujia and Joshi, Neel and Buehler, Christopher and Yang, Ming-Hsuan and Kang, Sing Bing},
    date = {2018-09},
    year = {2018},
    journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
    volume = {24},
    number = {9},
    pages = {2610--2621},
    issn = {1941-0506},
    doi = {10.1109/TVCG.2017.2750671},
    abstract = {We present a system for converting a fully panoramic (360 degree) video into a normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our system exploits visual saliency and semantics to non-uniformly sample in space and time for generating hyperlapses. In addition, users can optionally choose objects of interest for customizing the hyperlapses. We first stabilize an input 360 degree video by smoothing the rotation between adjacent frames and then compute regions of interest and saliency scores. An initial hyperlapse is generated by optimizing the saliency and motion smoothness followed by the saliency-aware frame selection. We further smooth the result using an efficient 2D video stabilization approach that adaptively selects the motion model to generate the final hyperlapse. We validate the design of our system by showing results for a variety of scenes and comparing against the state-of-the-art method through a large-scale user study.},
    eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
    keywords = {360 degree videos,Cameras,Computational modeling,hyperlapse,Rendering (computer graphics),semantic segmentation,Semantics,spatial-temporal saliency,Three-dimensional displays,Two dimensional displays,video stabilization,Visualization},
    file = {/home/oscar/Zotero/storage/GG5AF35Y/Lai et al. - 2018 - Semantic-Driven Generation of Hyperlapse from 360 .pdf;/home/oscar/Zotero/storage/CCRPP2LU/8031049.html}
}

@article{leeSpherePHDApplyingCNNs2019,
    title = {{{SpherePHD}}: {{Applying CNNs}} on a {{Spherical PolyHeDron Representation}} of 360 Degree {{Images}}},
    shorttitle = {{{SpherePHD}}},
    author = {Lee, Yeonkun and Jeong, Jaeseok and Yun, Jongseob and Cho, Wonjune and Yoon, Kuk-Jin},
    date = {2019-04-08},
    year = {2019},
    month = apr,
    eprint = {1811.08196},
    journal = {arXiv:1811.08196 [cs]},
    eprinttype = {arxiv},
    primaryclass = {cs},
    urldate = {2022-01-11},
    abstract = {Omni-directional cameras have many advantages overconventional cameras in that they have a much wider field-of-view (FOV). Accordingly, several approaches have beenproposed recently to apply convolutional neural networks(CNNs) to omni-directional images for various visual tasks.However, most of them use image representations defined inthe Euclidean space after transforming the omni-directionalviews originally formed in the non-Euclidean space. Thistransformation leads to shape distortion due to nonuniformspatial resolving power and the loss of continuity. Theseeffects make existing convolution kernels experience diffi-culties in extracting meaningful information.This paper presents a novel method to resolve such prob-lems of applying CNNs to omni-directional images. Theproposed method utilizes a spherical polyhedron to rep-resent omni-directional views. This method minimizes thevariance of the spatial resolving power on the sphere sur-face, and includes new convolution and pooling methodsfor the proposed representation. The proposed method canalso be adopted by any existing CNN-based methods. Thefeasibility of the proposed method is demonstrated throughclassification, detection, and semantic segmentation taskswith synthetic and real datasets.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/CS9Z6FLK/Lee et al. - 2019 - SpherePHD Applying CNNs on a Spherical PolyHeDron.pdf;/home/oscar/Zotero/storage/HQE3UPB6/1811.html}
}

@article{liApplyingVertexShuffle360Degree2021,
    title = {Applying {{VertexShuffle Toward}} 360-{{Degree Video Super-Resolution}} on {{Focused-Icosahedral-Mesh}}},
    author = {Li, Na and Liu, Yao},
    date = {2021-06-21},
    year = {2021},
    eprint = {2106.11253},
    eprinttype = {arxiv},
    primaryclass = {cs, eess},
    url = {http://arxiv.org/abs/2106.11253},
    urldate = {2022-01-11},
    abstract = {With the emerging of 360-degree image/video, augmented reality (AR) and virtual reality (VR), the demand for analysing and processing spherical signals get tremendous increase. However, plenty of effort paid on planar signals that projected from spherical signals, which leading to some problems, e.g. waste of pixels, distortion. Recent advances in spherical CNN have opened up the possibility of directly analysing spherical signals. However, they pay attention to the full mesh which makes it infeasible to deal with situations in real-world application due to the extremely large bandwidth requirement. To address the bandwidth waste problem associated with 360-degree video streaming and save computation, we exploit Focused Icosahedral Mesh to represent a small area and construct matrices to rotate spherical content to the focused mesh area. We also proposed a novel VertexShuffle operation that can significantly improve both the performance and the efficiency compared to the original MeshConv Transpose operation introduced in UGSCNN. We further apply our proposed methods on super resolution model, which is the first to propose a spherical super-resolution model that directly operates on a mesh representation of spherical pixels of 360-degree data. To evaluate our model, we also collect a set of high-resolution 360-degree videos to generate a spherical image dataset. Our experiments indicate that our proposed spherical super-resolution model achieves significant benefits in terms of both performance and inference time compared to the baseline spherical super-resolution model that uses the simple MeshConv Transpose operation. In summary, our model achieves great super-resolution performance on 360-degree inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices on the mesh.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Electrical Engineering and Systems Science - Image and Video Processing},
    file = {/home/oscar/Zotero/storage/AQVJPPK8/Li and Liu - 2021 - Applying VertexShuffle Toward 360-Degree Video Sup.pdf;/home/oscar/Zotero/storage/JIE84FR4/2106.html}
}

@article{linmansSampleEfficientSemantic2018a,
    title = {Sample {{{E}fficient {S}emantic {S}egmentation}} {U}sing {{{R}otation {E}quivariant {C}onvolutional {N}etworks}}},
    author = {Linmans, Jasper and Winkens, Jim and Veeling, Bastiaan S. and Cohen, Taco S. and Welling, Max},
    date = {2018-07-02},
    year = {2018},
    eprint = {1807.00583},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    url = {http://arxiv.org/abs/1807.00583},
    urldate = {2022-01-14},
    abstract = {We propose a semantic segmentation model that exploits rotation and reflection symmetries. We demonstrate significant gains in sample efficiency due to increased weight sharing, as well as improvements in robustness to symmetry transformations. The group equivariant CNN framework is extended for segmentation by introducing a new equivariant (G-{$>$}Z2)-convolution that transforms feature maps on a group to planar feature maps. Also, equivariant transposed convolution is formulated for up-sampling in an encoder-decoder network. To demonstrate improvements in sample efficiency we evaluate on multiple data regimes of a rotation-equivariant segmentation task: cancer metastases detection in histopathology images. We further show the effectiveness of exploiting more symmetries by varying the size of the group.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/479YBQNV/Linmans et al. - 2018 - Sample Efficient Semantic Segmentation using Rotat.pdf;/home/oscar/Zotero/storage/BFY664CD/1807.html}
}

@article{liuDEEPLEARNING3D2019,
    title = {{{DEEP LEARNING 3D SHAPES USING ALT-AZ ANISOTROPIC}} 2-{{SPHERE CONVOLUTION}}},
    author = {Liu, Min and Yao, Fupin and Choi, Chiho and Sinha, Ayan and Ramani, Karthik},
    date = {2019},
    year = {2019},
    pages = {14},
    abstract = {The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks. One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on noneuclidean surfaces. In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks. We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/U2BWLPNT/Liu et al. - 2019 - DEEP LEARNING 3D SHAPES USING ALT-AZ ANISOTROPIC 2.pdf}
}

@article{lohitRotationInvariantAutoencodersSignals2020,
    title = {Rotation-{{Invariant Autoencoders}} for {{Signals}} on {{Spheres}}},
    author = {Lohit, Suhas and Trivedi, Shubhendu},
    date = {2020-12-08},
    year = {2020},
    url = {https://arxiv.org/abs/2012.04474v1},
    urldate = {2022-01-11},
    abstract = {Omnidirectional images and spherical representations of \$3D\$ shapes cannot be processed with conventional 2D convolutional neural networks (CNNs) as the unwrapping leads to large distortion. Using fast implementations of spherical and \$SO(3)\$ convolutions, researchers have recently developed deep learning methods better suited for classifying spherical images. These newly proposed convolutional layers naturally extend the notion of convolution to functions on the unit sphere \$S\^2\$ and the group of rotations \$SO(3)\$ and these layers are equivariant to 3D rotations. In this paper, we consider the problem of unsupervised learning of rotation-invariant representations for spherical images. In particular, we carefully design an autoencoder architecture consisting of \$S\^2\$ and \$SO(3)\$ convolutional layers. As 3D rotations are often a nuisance factor, the latent space is constrained to be exactly invariant to these input transformations. As the rotation information is discarded in the latent space, we craft a novel rotation-invariant loss function for training the network. Extensive experiments on multiple datasets demonstrate the usefulness of the learned representations on clustering, retrieval and classification applications.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/ELS5L6LR/Lohit and Trivedi - 2020 - Rotation-Invariant Autoencoders for Signals on Sph.pdf;/home/oscar/Zotero/storage/SNHCXGV7/2012.html}
}

@article{longFullyConvolutionalNetworks2014,
    title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
    author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
    date = {2014-11-14},
    year = {2014},
    url = {https://arxiv.org/abs/1411.4038v2},
    urldate = {2022-01-13},
    abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/IRBY4BCK/Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/oscar/Zotero/storage/Q57FIFF9/1411.html}
}

@article{monroySalNet360SaliencyMaps2018,
    title = {{{SalNet360}}: {{Saliency}} Maps for Omni-Directional Images with {{CNN}}},
    shorttitle = {{{SalNet360}}},
    author = {Monroy, Rafael and Lutz, Sebastian and Chalasani, Tejo and Smolic, Aljosa},
    date = {2018-11-01},
    year = {2018},
    journaltitle = {Signal Processing: Image Communication},
    shortjournal = {Signal Processing: Image Communication},
    series = {Salient360: {{Visual}} Attention Modeling for 360° {{Images}}},
    volume = {69},
    pages = {26--34},
    issn = {0923-5965},
    doi = {10.1016/j.image.2018.05.005},
    url = {https://www.sciencedirect.com/science/article/pii/S0923596518304685},
    urldate = {2022-01-13},
    abstract = {The prediction of Visual Attention data from any kind of media is of valuable use to content creators and used to efficiently drive encoding algorithms. With the current trend in the Virtual Reality (VR) field, adapting known techniques to this new kind of media is starting to gain momentum. In this paper, we present an architectural extension to any Convolutional Neural Network (CNN) to fine-tune traditional 2D saliency prediction to Omnidirectional Images (ODIs) in an end-to-end manner. We show that each step in the proposed pipeline works towards making the generated saliency map more accurate with respect to ground truth data.},
    langid = {english},
    keywords = {Convolutional Neural Network (CNN),Omnidirectional Image (ODI),Saliency,Virtual Reality (VR)},
    file = {/home/oscar/Zotero/storage/84TKR4B8/Monroy et al. - 2018 - SalNet360 Saliency maps for omni-directional imag.pdf;/home/oscar/Zotero/storage/SH9MHCWA/S0923596518304685.html}
}

@article{purushwalkamDemystifyingContrastiveSelfSupervised2020,
    title = {Demystifying {{Contrastive Self-Supervised Learning}}: {{Invariances}}, {{Augmentations}} and {{Dataset Biases}}},
    shorttitle = {Demystifying {{Contrastive Self-Supervised Learning}}},
    author = {Purushwalkam, Senthil and Gupta, Abhinav},
    date = {2020-07-29},
    year = {2020},
    eprint = {2007.13916},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2007.13916},
    urldate = {2022-01-11},
    abstract = {Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/3SISBURW/Purushwalkam and Gupta - 2020 - Demystifying Contrastive Self-Supervised Learning.pdf;/home/oscar/Zotero/storage/4KY2QQ98/2007.html}
}

@article{ramachandranWoodscapeFisheyeSemantic2021,
    title = {Woodscape {{Fisheye Semantic Segmentation}} for {{Autonomous Driving}} -- {{CVPR}} 2021 {{OmniCV Workshop Challenge}}},
    author = {Ramachandran, Saravanabalagi and Sistu, Ganesh and McDonald, John and Yogamani, Senthil},
    date = {2021-07-17},
    year = {2021},
    eprint = {2107.08246},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2107.08246},
    urldate = {2022-01-14},
    abstract = {We present the WoodScape fisheye semantic segmentation challenge for autonomous driving which was held as part of the CVPR 2021 Workshop on Omnidirectional Computer Vision (OmniCV). This challenge is one of the first opportunities for the research community to evaluate the semantic segmentation techniques targeted for fisheye camera perception. Due to strong radial distortion standard models don't generalize well to fisheye images and hence the deformations in the visual appearance of objects and entities needs to be encoded implicitly or as explicit knowledge. This challenge served as a medium to investigate the challenges and new methodologies to handle the complexities with perception on fisheye images. The challenge was hosted on CodaLab and used the recently released WoodScape dataset comprising of 10k samples. In this paper, we provide a summary of the competition which attracted the participation of 71 global teams and a total of 395 submissions. The top teams recorded significantly improved mean IoU and accuracy scores over the baseline PSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms and analyze the failure cases. We conclude by providing future directions for the research.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
    file = {/home/oscar/Zotero/storage/3RY8RZQB/Ramachandran et al. - 2021 - Woodscape Fisheye Semantic Segmentation for Autono.pdf;/home/oscar/Zotero/storage/GQ2ILC3U/2107.html}
}

@article{rashedGeneralizedObjectDetection2020,
    title = {Generalized {{Object Detection}} on {{Fisheye Cameras}} for {{Autonomous Driving}}: {{Dataset}}, {{Representations}} and {{Baseline}}},
    shorttitle = {Generalized {{Object Detection}} on {{Fisheye Cameras}} for {{Autonomous Driving}}},
    author = {Rashed, Hazem and Mohamed, Eslam and Sistu, Ganesh and Kumar, Varun Ravi and Eising, Ciaran and El-Sallab, Ahmad and Yogamani, Senthil},
    date = {2020-12-03},
    year = {2020},
    eprint = {2012.02124},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2012.02124},
    urldate = {2022-01-14},
    abstract = {Object detection is a comprehensively studied problem in autonomous driving. However, it has been relatively less explored in the case of fisheye cameras. The standard bounding box fails in fisheye cameras due to the strong radial distortion, particularly in the image's periphery. We explore better representations like oriented bounding box, ellipse, and generic polygon for object detection in fisheye images in this work. We use the IoU metric to compare these representations using accurate instance segmentation ground truth. We design a novel curved bounding box model that has optimal properties for fisheye distortion models. We also design a curvature adaptive perimeter sampling method for obtaining polygon vertices, improving relative mAP score by 4.9\% compared to uniform sampling. Overall, the proposed polygon model improves mIoU relative accuracy by 40.3\%. It is the first detailed study on object detection on fisheye cameras for autonomous driving scenarios to the best of our knowledge. The dataset comprising of 10,000 images along with all the object representations ground truth will be made public to encourage further research. We summarize our work in a short video with qualitative results at https://youtu.be/iLkOzvJpL-A.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
    file = {/home/oscar/Zotero/storage/7ANXEHUB/Rashed et al. - 2020 - Generalized Object Detection on Fisheye Cameras fo.pdf;/home/oscar/Zotero/storage/LYW5GNGQ/2012.html}
}

@article{santosAvoidingOverfittingSurvey2022,
    title = {Avoiding {{Overfitting}}: {{A Survey}} on {{Regularization Methods}} for {{Convolutional Neural Networks}}},
    shorttitle = {Avoiding {{Overfitting}}},
    author = {dos Santos, Claudio Filipi Gonçalves and Papa, João Paulo},
    date = {2022-01-10},
    year = {2022},
    eprint = {2201.03299},
    eprinttype = {arxiv},
    primaryclass = {cs},
    doi = {10.1145/3510413},
    url = {http://arxiv.org/abs/2201.03299},
    urldate = {2022-01-11},
    abstract = {Several image processing tasks, such as image classification and object detection, have been significantly improved using Convolutional Neural Networks (CNN). Like ResNet and EfficientNet, many architectures have achieved outstanding results in at least one dataset by the time of their creation. A critical factor in training concerns the network's regularization, which prevents the structure from overfitting. This work analyzes several regularization methods developed in the last few years, showing significant improvements for different CNN models. The works are classified into three main areas: the first one is called "data augmentation", where all the techniques focus on performing changes in the input data. The second, named "internal changes", which aims to describe procedures to modify the feature maps generated by the neural network or the kernels. The last one, called "label", concerns transforming the labels of a given input. This work presents two main differences comparing to other available surveys about regularization: (i) the first concerns the papers gathered in the manuscript, which are not older than five years, and (ii) the second distinction is about reproducibility, i.e., all works refered here have their code available in public repositories or they have been directly implemented in some framework, such as TensorFlow or Torch.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
    file = {/home/oscar/Zotero/storage/YLRNV9KK/Santos and Papa - 2022 - Avoiding Overfitting A Survey on Regularization M.pdf;/home/oscar/Zotero/storage/7LQWFZPV/2201.html}
}


@InProceedings{pmlr-v139-shakerinava21a,
    title = 	 {{E}quivariant {N}etworks for {P}ixelized {S}pheres},
    author =       {Shakerinava, Mehran and Ravanbakhsh, Siamak},
    booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
    pages = 	 {9477--9488},
    year = 	 {2021},
    editor = 	 {Meila, Marina and Zhang, Tong},
    volume = 	 {139},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {18--24 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v139/shakerinava21a/shakerinava21a.pdf},
    abstract = 	 {Pixelizations of Platonic solids such as the cube and icosahedron have been widely used to represent spherical data, from climate records to Cosmic Microwave Background maps. Platonic solids have well-known global symmetries. Once we pixelize each face of the solid, each face also possesses its own local symmetries in the form of Euclidean isometries. One way to combine these symmetries is through a hierarchy. However, this approach does not adequately model the interplay between the two levels of symmetry transformations. We show how to model this interplay using ideas from group theory, identify the equivariant linear maps, and introduce equivariant padding that respects these symmetries. Deep networks that use these maps as their building blocks generalize gauge equivariant CNNs on pixelized spheres. These deep networks achieve state-of-the-art results on semantic segmentation for climate data and omnidirectional image processing. Code is available at https://git.io/JGiZA.}
}


@article{shakerinavaEquivariantNetworksPixelized2021,
    title = {Equivariant {{Networks}} for {{Pixelized Spheres}}},
    author = {Shakerinava, Mehran and Ravanbakhsh, Siamak},
    date = {2021-06-11},
    year = {2021},
    eprint = {2106.06662},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2106.06662},
    urldate = {2022-01-11},
    abstract = {Pixelizations of Platonic solids such as the cube and icosahedron have been widely used to represent spherical data, from climate records to Cosmic Microwave Background maps. Platonic solids have well-known global symmetries. Once we pixelize each face of the solid, each face also possesses its own local symmetries in the form of Euclidean isometries. One way to combine these symmetries is through a hierarchy. However, this approach does not adequately model the interplay between the two levels of symmetry transformations. We show how to model this interplay using ideas from group theory, identify the equivariant linear maps, and introduce equivariant padding that respects these symmetries. Deep networks that use these maps as their building blocks generalize gauge equivariant CNNs on pixelized spheres. These deep networks achieve state-of-the-art results on semantic segmentation for climate data and omnidirectional image processing. Code is available at https://git.io/JGiZA.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Machine Learning},
    file = {/home/oscar/Zotero/storage/X2IL6T7F/Shakerinava and Ravanbakhsh - 2021 - Equivariant Networks for Pixelized Spheres.pdf;/home/oscar/Zotero/storage/9BGVVKKC/2106.html}
}

@article{sharmaAugmentingImitationExperience2021,
    title = {Augmenting {{Imitation Experience}} via {{Equivariant Representations}}},
    author = {Sharma, Dhruv and Kuwajerwala, Alihusein and Shkurti, Florian},
    date = {2021-10-14},
    year = {2021},
    eprint = {2110.07668},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2110.07668},
    urldate = {2022-01-11},
    abstract = {The robustness of visual navigation policies trained through imitation often hinges on the augmentation of the training image-action pairs. Traditionally, this has been done by collecting data from multiple cameras, by using standard data augmentations from computer vision, such as adding random noise to each image, or by synthesizing training images. In this paper we show that there is another practical alternative for data augmentation for visual navigation based on extrapolating viewpoint embeddings and actions nearby the ones observed in the training data. Our method makes use of the geometry of the visual navigation problem in 2D and 3D and relies on policies that are functions of equivariant embeddings, as opposed to images. Given an image-action pair from a training navigation dataset, our neural network model predicts the latent representations of images at nearby viewpoints, using the equivariance property, and augments the dataset. We then train a policy on the augmented dataset. Our simulation results indicate that policies trained in this way exhibit reduced cross-track error, and require fewer interventions compared to policies trained using standard augmentation methods. We also show similar results in autonomous visual navigation by a real ground robot along a path of over 500m.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
    file = {/home/oscar/Zotero/storage/63U7JQ6G/Sharma et al. - 2021 - Augmenting Imitation Experience via Equivariant Re.pdf;/home/oscar/Zotero/storage/BET2JKAY/2110.html}
}

@article{shibataGeometricDataAugmentation2021,
    title = {Geometric {{Data Augmentation Based}} on {{Feature Map Ensemble}}},
    author = {Shibata, Takashi and Tanaka, Masayuki and Okutomi, Masatoshi},
    date = {2021-07-22},
    year = {2021},
    eprint = {2107.10524},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2107.10524},
    urldate = {2022-01-11},
    abstract = {Deep convolutional networks have become the mainstream in computer vision applications. Although CNNs have been successful in many computer vision tasks, it is not free from drawbacks. The performance of CNN is dramatically degraded by geometric transformation, such as large rotations. In this paper, we propose a novel CNN architecture that can improve the robustness against geometric transformations without modifying the existing backbones of their CNNs. The key is to enclose the existing backbone with a geometric transformation (and the corresponding reverse transformation) and a feature map ensemble. The proposed method can inherit the strengths of existing CNNs that have been presented so far. Furthermore, the proposed method can be employed in combination with state-of-the-art data augmentation algorithms to improve their performance. We demonstrate the effectiveness of the proposed method using standard datasets such as CIFAR, CUB-200, and Mnist-rot-12k.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/CR6IW279/Shibata et al. - 2021 - Geometric Data Augmentation Based on Feature Map E.pdf;/home/oscar/Zotero/storage/C8FN6JD5/2107.html}
}

@article{steinerHowTrainYour2021,
    title = {How to Train Your {{ViT}}? {{Data}}, {{Augmentation}}, and {{Regularization}} in {{Vision Transformers}}},
    shorttitle = {How to Train Your {{ViT}}?},
    author = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
    date = {2021-06-18},
    year = {2021},
    eprint = {2106.10270},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2106.10270},
    urldate = {2022-01-11},
    abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
    file = {/home/oscar/Zotero/storage/KU3JFXD7/Steiner et al. - 2021 - How to train your ViT Data, Augmentation, and Reg.pdf;/home/oscar/Zotero/storage/9A9FWSVJ/2106.html}
}

@inproceedings{suKernelTransformerNetworks2019,
    title = {Kernel {{Transformer Networks}} for {{Compact Spherical Convolution}}},
    booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    author = {Su, Yu-Chuan and Grauman, Kristen},
    date = {2019-06},
    year = {2019},
    pages = {9434--9443},
    publisher = {{IEEE}},
    location = {{Long Beach, CA, USA}},
    doi = {10.1109/CVPR.2019.00967},
    url = {https://ieeexplore.ieee.org/document/8953831/},
    urldate = {2022-01-13},
    abstract = {Ideally, 360◦ imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We present the Kernel Transformer Network (KTN) to efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360◦ images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360◦ image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN’s accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint1.},
    eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    isbn = {978-1-72813-293-8},
    langid = {english},
    file = {/home/oscar/Zotero/storage/H9J2ZVQD/Su and Grauman - 2019 - Kernel Transformer Networks for Compact Spherical .pdf}
}

@inproceedings{suLearningSphericalConvolution2017,
    title = {Learning {{Spherical Convolution}} for {{Fast Features}} from 360° {{Imagery}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Su, Yu-Chuan and Grauman, Kristen},
    date = {2017},
    year = {2017},
    volume = {30},
    publisher = {{Curran Associates, Inc.}},
    url = {https://proceedings.neurips.cc/paper/2017/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html},
    urldate = {2022-01-13},
    file = {/home/oscar/Zotero/storage/F2T8L9NX/Su and Grauman - 2017 - Learning Spherical Convolution for Fast Features f.pdf}
}

@incollection{tatenoDistortionAwareConvolutionalFilters2018,
    title = {Distortion-{{Aware Convolutional Filters}} for {{Dense Prediction}} in {{Panoramic Images}}},
    booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
    author = {Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
    editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
    date = {2018},
    year = {2018},
    series = {Lecture {{Notes}} in {{Computer Science}}},
    volume = {11220},
    pages = {732--750},
    publisher = {{Springer International Publishing}},
    location = {{Cham}},
    doi = {10.1007/978-3-030-01270-0_43},
    urldate = {2022-01-13},
    abstract = {There is a high demand of 3D data for 360◦ panoramic images and videos, pushed by the growing availability on the market of specialized hardware for both capturing (e.g., omni-directional cameras) as well as visualizing in 3D (e.g., head mounted displays) panoramic images and videos. At the same time, 3D sensors able to capture 3D panoramic data are expensive and/or hardly available. To fill this gap, we propose a learning approach for panoramic depth map estimation from a single image. Thanks to a specifically developed distortion-aware deformable convolution filter, our method can be trained by means of conventional perspective images, then used to regress depth for panoramic images, thus bypassing the effort needed to create annotated panoramic training dataset. We also demonstrate our approach for emerging tasks such as panoramic monocular SLAM, panoramic semantic segmentation and panoramic style transfer.},
    isbn = {978-3-030-01269-4 978-3-030-01270-0},
    langid = {english},
    file = {/home/oscar/Zotero/storage/RSHHH8MT/Tateno et al. - 2018 - Distortion-Aware Convolutional Filters for Dense P.pdf}
}

@article{trivediDiscriminativeLearningSimilarity2018,
    title = {Discriminative {{Learning}} of {{Similarity}} and {{Group Equivariant Representations}}},
    author = {Trivedi, Shubhendu},
    date = {2018-08-30},
    year = {2018},
    url = {https://arxiv.org/abs/1808.10078v1},
    urldate = {2022-01-11},
    abstract = {One of the most fundamental problems in machine learning is to compare examples: Given a pair of objects we want to return a value which indicates degree of (dis)similarity. Similarity is often task specific, and pre-defined distances can perform poorly, leading to work in metric learning. However, being able to learn a similarity-sensitive distance function also presupposes access to a rich, discriminative representation for the objects at hand. In this dissertation we present contributions towards both ends. In the first part of the thesis, assuming good representations for the data, we present a formulation for metric learning that makes a more direct attempt to optimize for the k-NN accuracy as compared to prior work. We also present extensions of this formulation to metric learning for kNN regression, asymmetric similarity learning and discriminative learning of Hamming distance. In the second part, we consider a situation where we are on a limited computational budget i.e. optimizing over a space of possible metrics would be infeasible, but access to a label aware distance metric is still desirable. We present a simple, and computationally inexpensive approach for estimating a well motivated metric that relies only on gradient estimates, discussing theoretical and experimental results. In the final part, we address representational issues, considering group equivariant convolutional neural networks (GCNNs). Equivariance to symmetry transformations is explicitly encoded in GCNNs; a classical CNN being the simplest example. In particular, we present a SO(3)-equivariant neural network architecture for spherical data, that operates entirely in Fourier space, while also providing a formalism for the design of fully Fourier neural networks that are equivariant to the action of any continuous compact group.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/WMLNTFBS/Trivedi - 2018 - Discriminative Learning of Similarity and Group Eq.pdf;/home/oscar/Zotero/storage/NV7EZ4BI/1808.html}
}

@article{veelingRotationEquivariantCNNs2018a,
    title = {Rotation {{Equivariant CNNs}} for {{Digital Pathology}}},
    author = {Veeling, Bastiaan S. and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
    date = {2018-06-08},
    year = {2018},
    eprint = {1806.03962},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    url = {http://arxiv.org/abs/1806.03962},
    urldate = {2022-01-14},
    abstract = {We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/SALA3ETX/Veeling et al. - 2018 - Rotation Equivariant CNNs for Digital Pathology.pdf;/home/oscar/Zotero/storage/LVA9LRKS/1806.html}
}

@article{wangSelfsupervisedEquivariantAttention2020,
    title = {Self-Supervised {{Equivariant Attention Mechanism}} for {{Weakly Supervised Semantic Segmentation}}},
    author = {Wang, Yude and Zhang, Jie and Kan, Meina and Shan, Shiguang and Chen, Xilin},
    date = {2020-04-09},
    year = {2020},
    eprint = {2004.04581},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2004.04581},
    urldate = {2022-01-11},
    abstract = {Image-level weakly supervised semantic segmentation is a challenging problem that has been deeply studied in recent years. Most of advanced solutions exploit class activation map (CAM). However, CAMs can hardly serve as the object mask due to the gap between full and weak supervisions. In this paper, we propose a self-supervised equivariant attention mechanism (SEAM) to discover additional supervision and narrow the gap. Our method is based on the observation that equivariance is an implicit constraint in fully supervised semantic segmentation, whose pixel-level labels take the same spatial transformation as the input images during data augmentation. However, this constraint is lost on the CAMs trained by image-level supervision. Therefore, we propose consistency regularization on predicted CAMs from various transformed images to provide self-supervision for network learning. Moreover, we propose a pixel correlation module (PCM), which exploits context appearance information and refines the prediction of current pixel by its similar neighbors, leading to further improvement on CAMs consistency. Extensive experiments on PASCAL VOC 2012 dataset demonstrate our method outperforms state-of-the-art methods using the same level of supervision. The code is released online.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/9MGGJENI/Wang et al. - 2020 - Self-supervised Equivariant Attention Mechanism fo.pdf;/home/oscar/Zotero/storage/ZZ7ZAMIS/2004.html}
}

@article{weiler3DSteerableCNNs2018b,
    title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
    author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
    date = {2018},
    year = {2018},
    journaltitle = {32nd Conference on Neural Information Processing Systems (NeurIPS)},
    pages = {12},
    abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
    langid = {english},
    file = {/home/oscar/Zotero/storage/VI9FIU9E/Weiler et al. - 3D Steerable CNNs Learning Rotationally Equivaria.pdf}
}

@article{yeUniversalSemanticSegmentation2020,
    title = {Universal {{Semantic Segmentation}} for {{Fisheye Urban Driving Images}}},
    author = {Ye, Yaozu and Yang, Kailun and Xiang, Kaite and Wang, Juan and Wang, Kaiwei},
    date = {2020-08-24},
    year = {2020},
    eprint = {2002.03736},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    url = {http://arxiv.org/abs/2002.03736},
    urldate = {2022-01-11},
    abstract = {Semantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye datasets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides a universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at https://github.com/Yaozhuwa/FisheyeSeg.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/3TLG5PB3/Ye et al. - 2020 - Universal Semantic Segmentation for Fisheye Urban .pdf;/home/oscar/Zotero/storage/X5ME7YP3/2002.html}
}

@article{zhangObjectAugObjectlevelData2021,
    title = {{{ObjectAug}}: {{Object-level Data Augmentation}} for {{Semantic Image Segmentation}}},
    shorttitle = {{{ObjectAug}}},
    author = {Zhang, Jiawei and Zhang, Yanchun and Xu, Xiaowei},
    date = {2021-04-21},
    year = {2021},
    eprint = {2102.00221},
    eprinttype = {arxiv},
    primaryclass = {cs},
    url = {http://arxiv.org/abs/2102.00221},
    urldate = {2022-01-11},
    abstract = {Semantic image segmentation aims to obtain object labels with precise boundaries, which usually suffers from overfitting. Recently, various data augmentation strategies like regional dropout and mix strategies have been proposed to address the problem. These strategies have proved to be effective for guiding the model to attend on less discriminative parts. However, current strategies operate at the image level, and objects and the background are coupled. Thus, the boundaries are not well augmented due to the fixed semantic scenario. In this paper, we propose ObjectAug to perform object-level augmentation for semantic image segmentation. ObjectAug first decouples the image into individual objects and the background using the semantic labels. Next, each object is augmented individually with commonly used augmentation methods (e.g., scaling, shifting, and rotation). Then, the black area brought by object augmentation is further restored using image inpainting. Finally, the augmented objects and background are assembled as an augmented image. In this way, the boundaries can be fully explored in the various semantic scenarios. In addition, ObjectAug can support category-aware augmentation that gives various possibilities to objects in each category, and can be easily combined with existing image-level augmentation methods to further boost performance. Comprehensive experiments are conducted on both natural image and medical image datasets. Experiment results demonstrate that our ObjectAug can evidently improve segmentation performance.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {/home/oscar/Zotero/storage/4ZXBPU2S/Zhang et al. - 2021 - ObjectAug Object-level Data Augmentation for Sema.pdf;/home/oscar/Zotero/storage/9Y87JAAY/2102.html}
}

@inproceedings{zhangOrientationAwareSemanticSegmentation2019,
    title = {Orientation-{{Aware Semantic Segmentation}} on {{Icosahedron Spheres}}},
    booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
    author = {Zhang, Chao and Liwicki, Stephan and Smith, William and Cipolla, Roberto},
    date = {2019-10},
    year = {2019},
    pages = {3532--3540},
    publisher = {{IEEE}},
    location = {{Seoul, Korea (South)}},
    doi = {10.1109/ICCV.2019.00363},
    urldate = {2022-01-25},
    abstract = {We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640×1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
    eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
    isbn = {978-1-72814-803-8},
    langid = {english},
    file = {/home/oscar/Zotero/storage/NGG3VZY3/Zhang et al. - 2019 - Orientation-Aware Semantic Segmentation on Icosahe.pdf}
}

@inproceedings{zhouOrientedResponseNetworks2017,
    title = {Oriented {{Response Networks}}},
    booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    author = {Zhou, Yanzhao and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
    date = {2017-07},
    year = {2017},
    pages = {4961--4970},
    publisher = {{IEEE}},
    location = {{Honolulu, HI}},
    doi = {10.1109/CVPR.2017.527},
    url = {http://ieeexplore.ieee.org/document/8100010/},
    urldate = {2022-01-13},
    abstract = {Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During backpropagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks 1.},
    eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
    isbn = {978-1-5386-0457-1},
    langid = {english},
    file = {/home/oscar/Zotero/storage/6WCGFTVE/Zhou et al. - 2017 - Oriented Response Networks.pdf}
}



@article{gerkenGeometricDeepLearning2021,
    title = {Geometric {{Deep Learning}} and {{Equivariant Neural Networks}}},
    author = {Gerken, Jan E. and Aronsson, Jimmy and Carlsson, Oscar and Linander, Hampus and Ohlsson, Fredrik and Petersson, Christoffer and Persson, Daniel},
    date = {2021-05-28},
    year = {2021},
    month = may,
    eprint = {2105.13926},
    eprinttype = {arxiv},
    journal = {arXiv:2105.13926 [hep-th, cs]},
    primaryclass = {hep-th},
    urldate = {2021-05-31},
    abstract = {We survey the mathematical foundations of geometric deep learning, focusing on group equivariant and gauge equivariant neural networks. We develop gauge equivariant convolutional neural networks on arbitrary manifolds \$\textbackslash mathcal\{M\}\$ using principal bundles with structure group \$K\$ and equivariant maps between sections of associated vector bundles. We also discuss group equivariant neural networks for homogeneous spaces \$\textbackslash mathcal\{M\}=G/K\$, which are instead equivariant with respect to the global symmetry \$G\$ on \$\textbackslash mathcal\{M\}\$. Group equivariant layers can be interpreted as intertwiners between induced representations of \$G\$, and we show their relation to gauge equivariant convolutional layers. We analyze several applications of this formalism, including semantic segmentation and object detection networks. We also discuss the case of spherical networks in great detail, corresponding to the case \$\textbackslash mathcal\{M\}=S\^2=\textbackslash mathrm\{SO\}(3)/\textbackslash mathrm\{SO\}(2)\$. Here we emphasize the use of Fourier analysis involving Wigner matrices, spherical harmonics and Clebsch-Gordan coefficients for \$G=\textbackslash mathrm\{SO\}(3)\$, illustrating the power of representation theory for deep learning.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,High Energy Physics - Theory},
    file = {/home/oscar/Zotero/storage/GG55DI3S/Gerken et al. - 2021 - Geometric Deep Learning and Equivariant Neural Net.pdf;/home/oscar/Zotero/storage/NJ62VZWM/2105.html}
}


@article{alphafold,
    title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
    author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
    date = {2021-08},
    year = {2021},
    journaltitle = {Nature},
    volume = {596},
    number = {7873},
    pages = {583--589},
    publisher = {{Nature Publishing Group}},
    issn = {1476-4687},
    doi = {10.1038/s41586-021-03819-2},
    url = {https://www.nature.com/articles/s41586-021-03819-2},
    urldate = {2022-01-19},
    abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50~years9. Despite recent progress10–14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
    issue = {7873},
    langid = {english},
    keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
    annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational biophysics;Machine learning;Protein structure predictions;Structural biology Subject\_term\_id: computational-biophysics;machine-learning;protein-structure-predictions;structural-biology},
    file = {/home/oscar/Zotero/storage/4KJV2TIJ/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf;/home/oscar/Zotero/storage/F5KYYAYP/s41586-021-03819-2.html}
}



@article{liEnhancedConvolutionalNeural2019,
    title = {Enhanced {{Convolutional Neural Tangent Kernels}}},
    author = {Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S. and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
    date = {2019-11-02},
    year = {2019},
    eprint = {1911.00809},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    journal = {arXiv:1911.00809 [cs, stat]},
    url = {http://arxiv.org/abs/1911.00809},
    urldate = {2022-01-20},
    abstract = {Recent research shows that for training with \$\textbackslash ell\_2\$ loss, convolutional neural networks (CNNs) whose width (number of channels in convolutional layers) goes to infinity correspond to regression with respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained, and correspond to regression with respect to the Convolutional Neural Tangent Kernel (CNTK) if all layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019) yielded the finding that classification accuracy of CNTK on CIFAR-10 is within 6-7\% of that of that of the corresponding CNN architecture (best figure being around 78\%) which is interesting performance for a fixed kernel. Here we show how to significantly enhance the performance of these kernels using two ideas. (1) Modifying the kernel using a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. Earlier papers were unable to incorporate naive data augmentation because of the quadratic training cost of kernel regression. This idea is inspired by Global Average Pooling (GAP), which we show for CNN-GP and CNTK is equivalent to full translation data augmentation. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP with LAP and horizontal flip data augmentation, achieves 89\% accuracy, matching the performance of AlexNet (Krizhevsky et al., 2012). Note that this is the best such result we know of for a classifier that is not a trained neural network. Similar improvements are obtained for Fashion-MNIST.},
    archiveprefix = {arXiv},
    keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
    file = {/home/oscar/Zotero/storage/XB5AK2JM/Li et al. - 2019 - Enhanced Convolutional Neural Tangent Kernels.pdf;/home/oscar/Zotero/storage/C26DIMGK/1911.html}
}


@article{DRISCOLL1994202,
    title = {Computing {F}ourier {T}ransforms and {C}onvolutions on the 2-Sphere},
    journal = {Advances in Applied Mathematics},
    volume = {15},
    number = {2},
    pages = {202-250},
    year = {1994},
    month = jun,
    issn = {0196-8858},
    doi = {https://doi.org/10.1006/aama.1994.1008},
    author = {J.R. Driscoll and D.M. Healy},
    abstract = {This paper considers the problem of efficient computation of the spherical harmonic expansion, or Fourier transform, of functions defined on the two dimensional sphere, S2. The resulting algorithms are applied to the efficient computation of convolutions of functions on the sphere. We begin by proving convolution theorems generalizing well known and useful results from the abelian case. These convolution theorems are then used to develop a sampling theorem on the sphere. which reduces the calculation of Fourier transforms and convolutions of band-limited functions to discrete computations. We show how to perform these efficiently, starting with an O(n(log n)2) time algorithm for computing the Legendre transform of a function defined on the interval [−1,1] sampled at n points there. Theoretical and experimental results on the effects of finite precision arithmetic are presented. The Legendre transform algorithm is then generalized to obtain an algorithm for the Fourier transform, requiring O(n(log n)2) time, and an algorithm for its inverse in O(n1.5) time, where n is the number of points on the sphere at which the function is sampled. This improves the naive O(n2) bound, which is the best previously known. These transforms give an O(n1.5) algorithm for convolving two functions on the sphere.}
}

@misc{kohlerSphericalCNNs2022,
    title = {Spherical {{CNNs}}},
    author = {Köhler, Jonas and Cohen, Taco S. and Geiger, Mario and Welling, Max},
    date = {2022-01-25T21:12:16Z},
    year = {2021},
    month = oct,
    origdate = {2017-09-23T16:49:11Z},
    url = {https://github.com/jonkhler/s2cnn},
    urldate = {2022-01-26},
    abstract = {Spherical CNNs}
}
